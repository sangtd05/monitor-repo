# SRE Best Practices & Monitoring Standards

## ğŸ“Š Tá»•ng quan

Site Reliability Engineering (SRE) lÃ  discipline káº¿t há»£p software engineering vÃ  operations Ä‘á»ƒ xÃ¢y dá»±ng vÃ  váº­n hÃ nh há»‡ thá»‘ng production Ä‘Ã¡ng tin cáº­y, cÃ³ kháº£ nÄƒng má»Ÿ rá»™ng.

TÃ i liá»‡u nÃ y tá»•ng há»£p cÃ¡c best practices vÃ  tiÃªu chuáº©n Ä‘Ã¡nh giÃ¡ cho há»‡ thá»‘ng monitoring.

---

## ğŸ¯ The Four Golden Signals (Google SRE)

Google SRE Ä‘á»‹nh nghÄ©a **4 metrics quan trá»ng nháº¥t** Ä‘á»ƒ monitor báº¥t ká»³ há»‡ thá»‘ng nÃ o:

### 1. **Latency** - Äá»™ trá»…

**Äá»‹nh nghÄ©a:** Thá»i gian Ä‘á»ƒ xá»­ lÃ½ má»™t request.

**Táº¡i sao quan trá»ng:**
- áº¢nh hÆ°á»Ÿng trá»±c tiáº¿p Ä‘áº¿n user experience
- PhÃ¡t hiá»‡n performance degradation sá»›m
- Indicator cá»§a resource contention

**Metrics:**
```promql
# HTTP request latency (p50, p95, p99)
histogram_quantile(0.95, 
  rate(http_request_duration_seconds_bucket[5m])
)

# Database query latency
rate(mongodb_ss_opLatencies_latency{type="reads"}[5m]) 
/ 
rate(mongodb_ss_opLatencies_ops{type="reads"}[5m])
```

**Best practices:**
- âœ… Track percentiles (p50, p95, p99), khÃ´ng chá»‰ average
- âœ… PhÃ¢n biá»‡t successful vs failed request latency
- âœ… Set SLO: "95% requests < 200ms"
- âš ï¸ Äá»«ng chá»‰ nhÃ¬n average (bá»‹ skew bá»Ÿi outliers)

---

### 2. **Traffic** - LÆ°u lÆ°á»£ng

**Äá»‹nh nghÄ©a:** Sá»‘ lÆ°á»£ng requests há»‡ thá»‘ng Ä‘ang xá»­ lÃ½.

**Táº¡i sao quan trá»ng:**
- Hiá»ƒu demand patterns
- Capacity planning
- PhÃ¡t hiá»‡n DDoS hoáº·c traffic spikes

**Metrics:**
```promql
# Requests per second
rate(http_requests_total[5m])

# Database operations per second
rate(mongodb_op_counters_total[5m])

# Active connections
pg_stat_activity_count{state="active"}
```

**Best practices:**
- âœ… Track by endpoint/service
- âœ… PhÃ¢n tÃ­ch traffic patterns (daily, weekly cycles)
- âœ… Alert trÃªn sudden changes (>50% increase)
- âœ… Correlate vá»›i business metrics (orders, signups)

---

### 3. **Errors** - Lá»—i

**Äá»‹nh nghÄ©a:** Tá»· lá»‡ requests tháº¥t báº¡i.

**Táº¡i sao quan trá»ng:**
- Trá»±c tiáº¿p áº£nh hÆ°á»Ÿng reliability
- Indicator cá»§a bugs hoáº·c infrastructure issues
- Critical cho SLA compliance

**Metrics:**
```promql
# Error rate
rate(http_requests_total{status=~"5.."}[5m]) 
/ 
rate(http_requests_total[5m])

# Database errors
rate(mongodb_ss_opcounters_repl_total{type="failed"}[5m])

# Application errors from logs
sum(rate({service="api", level="error"}[5m]))
```

**Best practices:**
- âœ… Track error rate (%), khÃ´ng chá»‰ absolute count
- âœ… Categorize errors: 4xx (client) vs 5xx (server)
- âœ… Alert trÃªn error rate > threshold (e.g., 1%)
- âœ… Include error details trong logs (stack traces)

---

### 4. **Saturation** - Äá»™ bÃ£o hÃ²a

**Äá»‹nh nghÄ©a:** Má»©c Ä‘á»™ "Ä‘áº§y" cá»§a resources (CPU, memory, disk, connections).

**Táº¡i sao quan trá»ng:**
- Predict khi nÃ o cáº§n scale
- PhÃ¡t hiá»‡n resource leaks
- Prevent outages trÆ°á»›c khi xáº£y ra

**Metrics:**
```promql
# CPU saturation
1 - avg(rate(node_cpu_seconds_total{mode="idle"}[5m]))

# Memory saturation
1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)

# Disk saturation
1 - (node_filesystem_avail_bytes / node_filesystem_size_bytes)

# Connection pool saturation
mongodb_connections{conn_type="current"} 
/ 
(mongodb_connections{conn_type="current"} + mongodb_connections{conn_type="available"})
```

**Best practices:**
- âœ… Monitor utilization AND queuing (wait time)
- âœ… Alert trÆ°á»›c khi Ä‘áº¡t 100% (e.g., 80%)
- âœ… Track trends Ä‘á»ƒ capacity planning
- âœ… Include headroom cho traffic spikes

---

## ğŸ“ˆ SLI, SLO, SLA Framework

### SLI (Service Level Indicator)

**Äá»‹nh nghÄ©a:** Metric Ä‘o lÆ°á»ng má»™t aspect cá»§a service level.

**Examples:**
```
Request latency: p95 < 200ms
Availability: % successful requests
Throughput: requests/second
Data durability: % data not lost
```

**Trong há»‡ thá»‘ng cá»§a báº¡n:**
```promql
# Availability SLI
sum(rate(http_requests_total{status!~"5.."}[5m])) 
/ 
sum(rate(http_requests_total[5m]))

# Latency SLI
histogram_quantile(0.95, 
  rate(http_request_duration_seconds_bucket[5m])
) < 0.2  # 200ms
```

---

### SLO (Service Level Objective)

**Äá»‹nh nghÄ©a:** Target value cho SLI.

**Format:** `[SLI] [comparison] [target] over [time window]`

**Examples:**
```
âœ… 99.9% of requests succeed (availability)
âœ… 95% of requests complete in < 200ms (latency)
âœ… 99% of data writes are durable (durability)
```

**Error Budget:**
```
SLO = 99.9% uptime
Error budget = 100% - 99.9% = 0.1%

Monthly error budget:
30 days Ã— 24 hours Ã— 60 minutes Ã— 0.1% = 43.2 minutes downtime/month
```

**Khi error budget cáº¡n kiá»‡t:**
- ğŸ›‘ Freeze feature releases
- ğŸ”§ Focus 100% on reliability
- ğŸ“Š Post-mortem vÃ  fix root causes

---

### SLA (Service Level Agreement)

**Äá»‹nh nghÄ©a:** Contract vá»›i customers vá» service level, cÃ³ consequences náº¿u vi pháº¡m.

**Example:**
```
SLA: 99.95% uptime
Consequence: 
  - < 99.95%: 10% credit
  - < 99.9%: 25% credit
  - < 99%: 100% credit
```

**Relationship:**
```
SLA (99.95%) < SLO (99.9%) < Internal target (99.99%)
     â†‘              â†‘                    â†‘
  Customer      Operational         Engineering
  commitment      target              goal
```

**Best practice:** SLO nÃªn stricter hÆ¡n SLA Ä‘á»ƒ cÃ³ buffer.

---

## ğŸ¨ Monitoring Best Practices

### 1. **USE Method** (Brendan Gregg)

Cho **resources** (CPU, disk, network):

- **Utilization:** % time resource is busy
- **Saturation:** Degree of queued work
- **Errors:** Error count

**Example:**
```promql
# CPU
Utilization: avg(rate(node_cpu_seconds_total{mode!="idle"}[5m]))
Saturation: node_load1 / count(node_cpu_seconds_total{mode="idle"})
Errors: node_cpu_guest_seconds_total (context switches)

# Disk
Utilization: rate(node_disk_io_time_seconds_total[5m])
Saturation: rate(node_disk_io_time_weighted_seconds_total[5m])
Errors: rate(node_disk_read_errors_total[5m])
```

---

### 2. **RED Method** (Tom Wilkie)

Cho **services** (APIs, microservices):

- **Rate:** Requests per second
- **Errors:** Failed requests per second
- **Duration:** Request latency

**Example:**
```promql
# Rate
sum(rate(http_requests_total[5m])) by (endpoint)

# Errors
sum(rate(http_requests_total{status=~"5.."}[5m])) by (endpoint)

# Duration
histogram_quantile(0.95, 
  sum(rate(http_request_duration_seconds_bucket[5m])) by (le, endpoint)
)
```

---

### 3. **Cardinality Management**

**Problem:** High cardinality labels â†’ memory explosion

**Bad:**
```promql
# user_id cÃ³ hÃ ng triá»‡u values
http_requests_total{user_id="12345"}  âŒ
```

**Good:**
```promql
# Chá»‰ dÃ¹ng labels cÃ³ bounded values
http_requests_total{endpoint="/api/users", status="200"}  âœ…

# user_id trong log content, khÃ´ng pháº£i label
{service="api"} | json | user_id="12345"  âœ…
```

**Rules:**
- âœ… Labels: Low cardinality (< 1000 unique values)
- âœ… Log content: High cardinality data
- âœ… Avoid: timestamps, IDs, emails trong labels

---

### 4. **Alert Design**

**Principles:**

**1. Actionable**
```yaml
# Bad: KhÃ´ng biáº¿t lÃ m gÃ¬
- alert: HighCPU
  expr: cpu > 80%
  
# Good: RÃµ rÃ ng action
- alert: HighCPUNeedScaling
  expr: cpu > 80%
  for: 15m
  annotations:
    summary: "CPU high for 15min, consider scaling"
    runbook: "https://wiki/runbooks/scale-cpu"
```

**2. Symptom-based, not cause-based**
```yaml
# Bad: Alert on cause
- alert: HighMemory
  expr: memory > 90%

# Good: Alert on symptom
- alert: SlowRequests
  expr: p95_latency > 500ms
  # Root cause cÃ³ thá»ƒ lÃ  memory, nhÆ°ng user care vá» latency
```

**3. Avoid alert fatigue**
```yaml
# Use "for" duration
for: 10m  # Chá» 10 phÃºt confirm váº¥n Ä‘á» thá»±c sá»±

# Use severity levels
severity: warning  # Page on-call
severity: info     # Ticket only
```

**4. Include context**
```yaml
annotations:
  summary: "{{ $labels.instance }} high error rate"
  description: |
    Error rate: {{ $value }}%
    Threshold: 5%
    Affected service: {{ $labels.service }}
    Dashboard: https://grafana/d/xyz
    Runbook: https://wiki/runbooks/high-errors
```

---

### 5. **Dashboard Design**

**Hierarchy:**

```
Level 1: Overview Dashboard
  â”œâ”€ Golden Signals (Latency, Traffic, Errors, Saturation)
  â”œâ”€ SLI/SLO status
  â””â”€ Top-level health indicators

Level 2: Service Dashboards
  â”œâ”€ Per-service metrics
  â”œâ”€ Dependencies
  â””â”€ Resource usage

Level 3: Deep Dive Dashboards
  â”œâ”€ Database internals
  â”œâ”€ Cache performance
  â””â”€ Network details
```

**Best practices:**
- âœ… Start with overview, drill down khi cáº§n
- âœ… Use consistent color scheme (red = bad, green = good)
- âœ… Include links to runbooks
- âœ… Show trends (day-over-day, week-over-week)
- âœ… Avoid clutter: < 10 panels per dashboard

---

## ğŸ” Evaluation Criteria

### 1. **Coverage** - Äá»™ bao phá»§

**Questions:**
- âœ… CÃ³ monitor táº¥t cáº£ critical services?
- âœ… CÃ³ monitor dependencies (databases, caches, queues)?
- âœ… CÃ³ monitor infrastructure (CPU, memory, disk, network)?
- âœ… CÃ³ collect logs tá»« táº¥t cáº£ components?

**Scoring:**
```
Excellent: 90-100% coverage
Good: 70-89%
Fair: 50-69%
Poor: < 50%
```

---

### 2. **Observability Maturity**

**Level 1: Reactive** âŒ
- Chá»‰ biáº¿t cÃ³ váº¥n Ä‘á» khi users complain
- KhÃ´ng cÃ³ metrics/logs
- Manual troubleshooting

**Level 2: Proactive** âš ï¸
- Basic metrics (CPU, memory)
- Alerts on thresholds
- Logs available nhÆ°ng khÃ´ng structured

**Level 3: Predictive** âœ…
- Comprehensive metrics (Golden Signals)
- SLI/SLO tracking
- Structured logs vá»›i correlation IDs
- Dashboards cho má»i service

**Level 4: Autonomous** ğŸš€
- Auto-remediation
- Anomaly detection (ML)
- Distributed tracing
- Full correlation: metrics â†” traces â†” logs

**Há»‡ thá»‘ng cá»§a báº¡n:** Level 3-4 (cÃ³ metrics, logs, traces via SigNoz)

---

### 3. **MTTD & MTTR**

**MTTD (Mean Time To Detect):**
- Thá»i gian tá»« khi incident xáº£y ra â†’ phÃ¡t hiá»‡n
- Target: < 5 minutes

**MTTR (Mean Time To Resolve):**
- Thá»i gian tá»« phÃ¡t hiá»‡n â†’ resolve
- Target: < 30 minutes (critical), < 4 hours (non-critical)

**Improvement strategies:**
- âœ… Better alerts â†’ Reduce MTTD
- âœ… Runbooks â†’ Reduce MTTR
- âœ… Auto-remediation â†’ Reduce MTTR to seconds

---

### 4. **Alert Quality**

**Metrics:**

**Precision:**
```
Precision = True Positives / (True Positives + False Positives)

Target: > 90%
```

**Recall:**
```
Recall = True Positives / (True Positives + False Negatives)

Target: > 95%
```

**Alert Fatigue Index:**
```
Fatigue = Alerts per day / On-call engineers

Good: < 5 alerts/day/person
Bad: > 20 alerts/day/person
```

---

### 5. **Data Retention & Cost**

**Retention tiers:**

| Tier | Duration | Resolution | Use case |
|------|----------|------------|----------|
| **Raw** | 7-15 days | Full | Recent debugging |
| **Downsampled** | 30-90 days | 5min avg | Trend analysis |
| **Aggregated** | 1-2 years | 1hour avg | Long-term planning |

**Cost optimization:**
```
Loki (label-based): $10-50/TB/month
vs
Elasticsearch (full-text): $100-500/TB/month

Savings: 10-50x
```

---

## ğŸ“‹ Checklist: ÄÃ¡nh giÃ¡ Há»‡ thá»‘ng Monitoring

### âœ… Metrics

- [ ] Collect Golden Signals (Latency, Traffic, Errors, Saturation)
- [ ] Track SLI/SLO cho critical services
- [ ] Use percentiles (p50, p95, p99), khÃ´ng chá»‰ averages
- [ ] Low cardinality labels (< 1000 unique values)
- [ ] Retention policy defined (15 days raw + downsampling)

### âœ… Logs

- [ ] Structured logging (JSON)
- [ ] Include correlation IDs (trace_id, request_id)
- [ ] Log levels consistent (DEBUG, INFO, WARN, ERROR)
- [ ] Sensitive data khÃ´ng log (passwords, tokens)
- [ ] Retention policy (7-30 days)

### âœ… Traces

- [ ] Distributed tracing enabled
- [ ] Sampling strategy defined (e.g., 10%)
- [ ] Trace context propagation (W3C Trace Context)
- [ ] Link traces â†” logs via trace_id

### âœ… Alerts

- [ ] Symptom-based, khÃ´ng chá»‰ cause-based
- [ ] Actionable (cÃ³ runbook)
- [ ] Avoid alert fatigue (< 5 alerts/day/person)
- [ ] Include context (dashboard links, affected resources)
- [ ] Test alerts regularly

### âœ… Dashboards

- [ ] Overview dashboard (Golden Signals)
- [ ] Per-service dashboards
- [ ] SLI/SLO tracking dashboard
- [ ] Consistent design (colors, layouts)
- [ ] Links to runbooks

### âœ… Operational

- [ ] On-call rotation defined
- [ ] Runbooks cho common incidents
- [ ] Post-mortem process
- [ ] Regular review cá»§a alerts (remove noise)
- [ ] Capacity planning based on trends

---

## ğŸ“ Ãp dá»¥ng cho Há»‡ thá»‘ng cá»§a báº¡n

### Hiá»‡n táº¡i báº¡n cÃ³:

**âœ… Strengths:**
- Comprehensive metrics (Prometheus)
- Structured logs (Loki + Promtail)
- Distributed tracing (SigNoz)
- Alert rules (Alertmanager â†’ Telegram)
- Database monitoring (PostgreSQL, MongoDB exporters)
- Infrastructure monitoring (Node Exporter, cAdvisor)

**âš ï¸ Cáº§n improve:**

1. **Define SLI/SLO:**
```yaml
# Example SLO
SLO:
  - name: API Availability
    target: 99.9%
    window: 30d
    
  - name: API Latency
    target: p95 < 200ms
    window: 30d
```

2. **Create runbooks:**
```markdown
# Runbook: High Error Rate

## Symptoms
- Error rate > 5%
- Alert: HighErrorRate firing

## Investigation
1. Check Grafana dashboard: [link]
2. View recent errors in Loki:
   {service="api", level="error"} [5m]
3. Check traces in SigNoz for failed requests

## Common causes
- Database connection pool exhausted
- External API timeout
- Deployment issue

## Resolution
- Scale database connections
- Rollback deployment
- Contact external API provider
```

3. **Implement error budget tracking:**
```promql
# Error budget remaining
(1 - (
  sum(rate(http_requests_total{status=~"5.."}[30d])) 
  / 
  sum(rate(http_requests_total[30d]))
)) - 0.999  # SLO = 99.9%
```

4. **Add business metrics:**
```promql
# Orders per minute
rate(orders_total[5m])

# Revenue per hour
sum(rate(order_revenue_total[1h]))

# User signups
rate(user_signups_total[5m])
```

---

## ğŸ“š Resources

**Books:**
- "Site Reliability Engineering" - Google
- "The Site Reliability Workbook" - Google
- "Observability Engineering" - Charity Majors

**Links:**
- [Google SRE Book](https://sre.google/sre-book/table-of-contents/)
- [Prometheus Best Practices](https://prometheus.io/docs/practices/)
- [Grafana Loki Best Practices](https://grafana.com/docs/loki/latest/best-practices/)

---

## ğŸ¯ Summary

**Key Takeaways:**

1. **Monitor Golden Signals:** Latency, Traffic, Errors, Saturation
2. **Define SLI/SLO:** Measure what matters to users
3. **Alert on symptoms:** Not causes
4. **Reduce MTTD & MTTR:** Faster detection & resolution
5. **Avoid alert fatigue:** Quality over quantity
6. **Correlation is key:** Metrics â†” Traces â†” Logs
7. **Continuous improvement:** Review vÃ  optimize regularly

**Your monitoring stack is solid!** Focus on:
- Defining SLI/SLO
- Creating runbooks
- Tracking error budgets
- Adding business metrics
