# SRE Best Practices - Giáº£i thÃ­ch Chi tiáº¿t

## ğŸ“– Má»¥c Ä‘Ã­ch tÃ i liá»‡u

TÃ i liá»‡u nÃ y giáº£i thÃ­ch **Táº I SAO** vÃ  **LÃ€M THáº¾ NÃ€O** Ã¡p dá»¥ng cÃ¡c SRE best practices, khÃ´ng chá»‰ liá»‡t kÃª. Má»—i practice Ä‘i kÃ¨m vá»›i:
- âœ… Giáº£i thÃ­ch táº¡i sao quan trá»ng
- ğŸ¯ VÃ­ dá»¥ thá»±c táº¿
- ğŸ› ï¸ CÃ¡ch implement
- âš ï¸ Pitfalls cáº§n trÃ¡nh

---

## 1. The Four Golden Signals - Giáº£i thÃ­ch Thá»±c táº¿

### Táº¡i sao chá»‰ 4 signals?

Google SRE team phÃ¡t hiá»‡n ráº±ng **80% váº¥n Ä‘á»** cÃ³ thá»ƒ phÃ¡t hiá»‡n qua 4 metrics nÃ y. Thay vÃ¬ monitor hÃ ng trÄƒm metrics, focus vÃ o 4 cÃ¡i quan trá»ng nháº¥t.

### 1.1 Latency - Hiá»ƒu SÃ¢u

**CÃ¢u chuyá»‡n thá»±c táº¿:**

```
Scenario: E-commerce website
- Average latency: 100ms âœ… Looks good!
- NhÆ°ng users váº«n complain "trang cháº­m"

Váº¥n Ä‘á»: Average bá»‹ skew bá»Ÿi majority fast requests
Reality:
  - p50 (median): 50ms    â† 50% users OK
  - p95: 500ms            â† 5% users cháº­m
  - p99: 2000ms           â† 1% users Ráº¤T cháº­m
  
Impact: 1% = 10,000 users/day náº¿u cÃ³ 1M users
```

**Lesson:** Äá»«ng chá»‰ nhÃ¬n average. Track percentiles!

**Implementation:**

```promql
# BAD: Average latency
avg(http_request_duration_seconds)

# GOOD: Percentiles
histogram_quantile(0.50, rate(http_request_duration_seconds_bucket[5m]))  # p50
histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))  # p95
histogram_quantile(0.99, rate(http_request_duration_seconds_bucket[5m]))  # p99
```

**Táº¡i sao phÃ¢n biá»‡t successful vs failed requests?**

```
Scenario: API timeout sau 30s
- Failed requests: 30s latency (timeout)
- Successful requests: 100ms latency

Náº¿u mix cáº£ 2:
  Average = (100ms Ã— 99 + 30000ms Ã— 1) / 100 = 400ms
  
Misleading! Successful requests váº«n nhanh, nhÆ°ng cÃ³ 1% timeout.
```

**Best practice:**

```promql
# Latency cá»§a successful requests only
histogram_quantile(0.95, 
  rate(http_request_duration_seconds_bucket{status!~"5.."}[5m])
)

# Latency cá»§a failed requests separately
histogram_quantile(0.95, 
  rate(http_request_duration_seconds_bucket{status=~"5.."}[5m])
)
```

---

### 1.2 Traffic - Táº¡i sao cáº§n monitor?

**CÃ¢u chuyá»‡n thá»±c táº¿:**

```
Scenario: Black Friday sale
- Normal traffic: 1000 req/s
- Black Friday: 50,000 req/s (50x increase)

KhÃ´ng monitor traffic:
  âŒ Servers crash vÃ¬ khÃ´ng scale ká»‹p
  âŒ Database connection pool exhausted
  âŒ Máº¥t revenue vÃ¬ downtime

CÃ³ monitor traffic:
  âœ… Alert khi traffic > 10,000 req/s
  âœ… Auto-scale thÃªm servers
  âœ… Pre-warm caches
  âœ… Increase connection pools
```

**Pattern Recognition:**

```promql
# Daily pattern
rate(http_requests_total[5m])

Typical pattern:
  00:00-06:00: 100 req/s   (night)
  06:00-09:00: 500 req/s   (morning rush)
  09:00-18:00: 1000 req/s  (work hours)
  18:00-22:00: 800 req/s   (evening)
  22:00-00:00: 200 req/s   (late night)
```

**Anomaly Detection:**

```promql
# Alert náº¿u traffic tÄƒng Ä‘á»™t ngá»™t > 50%
(
  rate(http_requests_total[5m])
  /
  rate(http_requests_total[5m] offset 1h)
) > 1.5
```

**Giáº£i thÃ­ch:**
- `rate(...[5m])`: Traffic hiá»‡n táº¡i
- `rate(...[5m] offset 1h)`: Traffic 1 giá» trÆ°á»›c
- Ratio > 1.5 = tÄƒng 50%

**Use cases:**
- DDoS attack detection
- Marketing campaign impact
- Viral content spike
- Bot traffic

---

### 1.3 Errors - Táº¡i sao Error Rate quan trá»ng hÆ¡n Error Count?

**VÃ­ dá»¥ thá»±c táº¿:**

```
Scenario 1: Low traffic
- 10 requests/minute
- 1 error/minute
- Error count: 1 âŒ Looks OK
- Error rate: 10% âš ï¸ VERY BAD!

Scenario 2: High traffic
- 10,000 requests/minute
- 100 errors/minute
- Error count: 100 âŒ Looks BAD
- Error rate: 1% âœ… Acceptable

Lesson: Error rate cho context, error count khÃ´ng
```

**Implementation:**

```promql
# Error rate (percentage)
(
  rate(http_requests_total{status=~"5.."}[5m])
  /
  rate(http_requests_total[5m])
) * 100

# Alert rule
- alert: HighErrorRate
  expr: |
    (
      rate(http_requests_total{status=~"5.."}[5m])
      /
      rate(http_requests_total[5m])
    ) > 0.01  # 1%
  for: 5m
  annotations:
    summary: "Error rate > 1% for 5 minutes"
```

**PhÃ¢n loáº¡i Errors:**

```
4xx Errors (Client errors):
  - 400 Bad Request: Client gá»­i invalid data
  - 401 Unauthorized: Authentication failed
  - 404 Not Found: Resource khÃ´ng tá»“n táº¡i
  
  â†’ ThÆ°á»ng KHÃ”NG pháº£i lá»—i há»‡ thá»‘ng
  â†’ CÃ³ thá»ƒ do user error hoáº·c bot
  â†’ Alert náº¿u spike Ä‘á»™t ngá»™t (cÃ³ thá»ƒ lÃ  attack)

5xx Errors (Server errors):
  - 500 Internal Server Error: Bug trong code
  - 502 Bad Gateway: Upstream service down
  - 503 Service Unavailable: Overloaded
  
  â†’ LÃ€ lá»—i há»‡ thá»‘ng
  â†’ Cáº¦N alert vÃ  fix ngay
  â†’ áº¢nh hÆ°á»Ÿng SLA
```

**Best practice:**

```promql
# Track separately
4xx_rate = rate(http_requests_total{status=~"4.."}[5m]) / rate(http_requests_total[5m])
5xx_rate = rate(http_requests_total{status=~"5.."}[5m]) / rate(http_requests_total[5m])

# Alert chá»‰ trÃªn 5xx
- alert: ServerErrors
  expr: 5xx_rate > 0.01
```

---

### 1.4 Saturation - Predict TrÆ°á»›c Khi QuÃ¡ Muá»™n

**CÃ¢u chuyá»‡n thá»±c táº¿:**

```
Scenario: Database connection pool
- Max connections: 100
- Current: 95
- Utilization: 95% âš ï¸

KhÃ´ng monitor saturation:
  âŒ Request thá»© 101 â†’ wait
  âŒ Request thá»© 102 â†’ wait
  âŒ Queue builds up
  âŒ Timeouts cascade
  âŒ Total outage

CÃ³ monitor saturation:
  âœ… Alert at 80% (cÃ²n buffer 20 connections)
  âœ… Scale database hoáº·c increase pool
  âœ… Prevent outage
```

**Metrics cho Saturation:**

```promql
# Connection pool saturation
mongodb_connections{conn_type="current"} 
/ 
(mongodb_connections{conn_type="current"} + mongodb_connections{conn_type="available"})

Interpretation:
  < 0.5 (50%): Healthy
  0.5-0.7 (50-70%): Normal load
  0.7-0.9 (70-90%): High load, monitor closely
  > 0.9 (90%): CRITICAL, scale now!
```

**CPU Saturation - KhÃ´ng chá»‰ lÃ  Utilization:**

```
CPU Utilization: % time CPU is busy
  - 80% CPU â†’ CÃ³ thá»ƒ OK náº¿u khÃ´ng cÃ³ queuing

CPU Saturation: Work waiting in queue
  - Load average > CPU cores â†’ CÃ³ work Ä‘ang chá»
  
Example:
  4-core machine
  Load average: 8.0
  â†’ 4 cores Ä‘ang cháº¡y, 4 tasks Ä‘ang chá»
  â†’ Saturation = 100%
```

```promql
# CPU saturation
node_load1 / count(node_cpu_seconds_total{mode="idle"})

Interpretation:
  < 1.0: No saturation
  1.0-2.0: Some queuing
  > 2.0: Significant queuing, scale!
```

**Disk Saturation:**

```promql
# Disk space saturation
1 - (node_filesystem_avail_bytes / node_filesystem_size_bytes)

# Alert at 80%, not 95%!
- alert: DiskSpaceLow
  expr: |
    1 - (node_filesystem_avail_bytes / node_filesystem_size_bytes) > 0.8
  for: 15m
  annotations:
    summary: "Disk {{ $labels.device }} is 80% full"
    description: "Only {{ $value | humanizePercentage }} space remaining"
```

**Táº¡i sao alert á»Ÿ 80%, khÃ´ng pháº£i 95%?**

```
Scenario: Log files growing
- Disk: 1TB
- Current: 800GB (80%)
- Growth rate: 50GB/day

At 80% alert:
  â†’ 200GB remaining
  â†’ 4 days to full
  â†’ Time to cleanup or expand

At 95% alert:
  â†’ 50GB remaining
  â†’ 1 day to full
  â†’ PANIC MODE!
```

---

## 2. SLI/SLO/SLA - Giáº£i thÃ­ch Thá»±c táº¿

### Táº¡i sao cáº§n SLI/SLO?

**Váº¥n Ä‘á» khÃ´ng cÃ³ SLO:**

```
PM: "Há»‡ thá»‘ng cÃ³ á»•n Ä‘á»‹nh khÃ´ng?"
Dev: "á»”n Ä‘á»‹nh"
PM: "á»”n Ä‘á»‹nh lÃ  sao?"
Dev: "á»ª... khÃ´ng cÃ³ downtime nhiá»u"
PM: "Nhiá»u lÃ  bao nhiÃªu?"
Dev: "..."

â†’ KhÃ´ng cÃ³ tiÃªu chuáº©n rÃµ rÃ ng
â†’ KhÃ´ng biáº¿t khi nÃ o cáº§n improve
â†’ KhÃ´ng cÃ³ data Ä‘á»ƒ justify resources
```

**Vá»›i SLO:**

```
SLO: 99.9% uptime = 43.2 minutes downtime/month

Month 1: 99.95% uptime (21.6 min downtime) âœ… Beat SLO
Month 2: 99.85% uptime (64.8 min downtime) âŒ Miss SLO

Action: Post-mortem, fix root causes, improve
```

### SLI Examples - Thá»±c táº¿

**1. Availability SLI:**

```promql
# Definition
availability = successful_requests / total_requests

# Implementation
sum(rate(http_requests_total{status!~"5.."}[30d]))
/
sum(rate(http_requests_total[30d]))

# Example values
0.999 = 99.9% = 43.2 min downtime/month
0.9999 = 99.99% = 4.32 min downtime/month
0.99999 = 99.999% = 26 seconds downtime/month
```

**Táº¡i sao 30 days window?**

```
Short window (1 hour):
  - Spike trong 1 giá» â†’ SLI drop dramatically
  - KhÃ´ng reflect overall reliability
  
Long window (30 days):
  - Smooth out short-term issues
  - Reflect user experience over time
  - Align vá»›i monthly SLA
```

**2. Latency SLI:**

```promql
# Definition
latency_sli = percentage of requests < threshold

# Implementation
sum(rate(http_request_duration_seconds_bucket{le="0.2"}[30d]))
/
sum(rate(http_request_duration_seconds_count[30d]))

# Interpretation
0.95 = 95% of requests < 200ms
```

**Táº¡i sao p95, khÃ´ng pháº£i p99?**

```
p95: 95% users cÃ³ experience tá»‘t
  - 5% users = 50,000 users/day (náº¿u 1M users)
  - Äá»§ lá»›n Ä‘á»ƒ care
  - Achievable target

p99: 99% users cÃ³ experience tá»‘t
  - 1% users = 10,000 users/day
  - Harder to achieve
  - Expensive to optimize (long tail)
  
Balance: p95 cho most users, p99 cho critical paths
```

### Error Budget - Giáº£i thÃ­ch Thá»±c táº¿

**Concept:**

```
SLO = 99.9% uptime
Error budget = 100% - 99.9% = 0.1%

Monthly error budget:
  30 days Ã— 24 hours Ã— 60 minutes = 43,200 minutes
  43,200 Ã— 0.1% = 43.2 minutes downtime allowed
```

**Táº¡i sao Error Budget quan trá»ng?**

**Scenario 1: KhÃ´ng cÃ³ Error Budget**

```
Dev team: "ChÃºng ta deploy feature má»›i nhÃ©"
Ops team: "KhÃ´ng, rá»§i ro quÃ¡"
Dev team: "NhÆ°ng customers cáº§n feature nÃ y"
Ops team: "NhÆ°ng stability..."

â†’ Conflict giá»¯a innovation vÃ  stability
â†’ KhÃ´ng cÃ³ data Ä‘á»ƒ quyáº¿t Ä‘á»‹nh
```

**Scenario 2: CÃ³ Error Budget**

```
Current error budget: 30 minutes remaining (70% used)

Dev team: "Deploy feature má»›i?"
Ops team: "Check error budget... cÃ²n 30 minutes"
Decision: 
  - If feature critical: Deploy, accept risk
  - If feature nice-to-have: Wait until next month
  
â†’ Data-driven decision
â†’ Balance innovation vÃ  stability
```

**Error Budget Policy:**

```yaml
Error Budget Status: Actions

> 50% remaining:
  âœ… Normal velocity
  âœ… Deploy new features
  âœ… Experiments OK

10-50% remaining:
  âš ï¸ Slow down deployments
  âš ï¸ Focus on reliability
  âš ï¸ No risky experiments

< 10% remaining:
  ğŸ›‘ FREEZE deployments
  ğŸ›‘ Only critical fixes
  ğŸ›‘ All hands on reliability
  ğŸ›‘ Post-mortem required

0% (exhausted):
  ğŸš¨ INCIDENT
  ğŸš¨ Immediate action
  ğŸš¨ Executive escalation
```

**Tracking Error Budget:**

```promql
# Error budget remaining (percentage)
error_budget_remaining = (
  1 - (
    sum(rate(http_requests_total{status=~"5.."}[30d]))
    /
    sum(rate(http_requests_total[30d]))
  )
) - slo_target

# Example
SLO = 0.999 (99.9%)
Current availability = 0.9985 (99.85%)

Error budget used:
  (1 - 0.9985) - (1 - 0.999) = 0.0015 - 0.001 = 0.0005
  = 0.05% of error budget used
  = 50% of allowed errors

Error budget remaining: 50%
```

---

## 3. Alert Design - Thá»±c táº¿

### Váº¥n Ä‘á» Alert Fatigue

**CÃ¢u chuyá»‡n thá»±c táº¿:**

```
Week 1: 50 alerts/day
  â†’ On-call engineer checks má»—i alert
  â†’ 45 alerts false positive
  â†’ 5 alerts tháº­t

Week 2: 50 alerts/day
  â†’ Engineer báº¯t Ä‘áº§u ignore
  â†’ Miss 1 critical alert
  â†’ Outage 2 hours

Week 3: 50 alerts/day
  â†’ Engineer mute alerts
  â†’ Miss major incident
  â†’ Outage 6 hours

Lesson: Too many alerts = No alerts
```

**Solution: Alert Quality over Quantity**

```yaml
# BAD: Alert on everything
- alert: HighCPU
  expr: cpu > 50%  # Fires constantly
  
- alert: AnyError
  expr: errors > 0  # Always firing

# GOOD: Alert on symptoms that matter
- alert: SlowUserExperience
  expr: p95_latency > 500ms
  for: 10m  # Confirm it's real
  annotations:
    summary: "Users experiencing slow response"
    impact: "Affects checkout flow"
    runbook: "https://wiki/runbooks/slow-response"
```

### Actionable Alerts

**BAD Alert:**

```yaml
- alert: HighMemory
  expr: memory > 90%
  annotations:
    summary: "Memory is high"
```

**Váº¥n Ä‘á»:**
- KhÃ´ng biáº¿t lÃ m gÃ¬
- KhÃ´ng biáº¿t impact
- KhÃ´ng biáº¿t urgent hay khÃ´ng

**GOOD Alert:**

```yaml
- alert: MemoryLeakDetected
  expr: |
    (
      node_memory_MemAvailable_bytes
      /
      node_memory_MemTotal_bytes
    ) < 0.1
  for: 15m
  labels:
    severity: critical
    component: infrastructure
  annotations:
    summary: "{{ $labels.instance }} memory critically low"
    description: |
      Memory available: {{ $value | humanizePercentage }}
      
      Impact:
        - OOM killer may start
        - Application crashes possible
        - User requests will fail
      
      Immediate actions:
        1. Check for memory leaks: kubectl top pods
        2. Restart leaking pods
        3. Scale up if needed
      
      Runbook: https://wiki/runbooks/memory-leak
      Dashboard: https://grafana/d/memory
      Slack: #oncall-critical
```

**Táº¡i sao tá»‘t hÆ¡n:**
- âœ… RÃµ rÃ ng severity (critical)
- âœ… Explain impact (OOM, crashes)
- âœ… Actionable steps (check, restart, scale)
- âœ… Links to resources (runbook, dashboard)

### Symptom vs Cause Based Alerts

**Concept:**

```
Cause: NguyÃªn nhÃ¢n (CPU high, memory high, disk full)
Symptom: Triá»‡u chá»©ng (slow response, errors, timeouts)

Users care vá» symptoms, khÃ´ng pháº£i causes
```

**Example:**

```yaml
# BAD: Cause-based
- alert: HighCPU
  expr: cpu > 80%
  # Váº¥n Ä‘á»: CPU 80% cÃ³ thá»ƒ OK náº¿u requests váº«n nhanh

# GOOD: Symptom-based
- alert: SlowRequests
  expr: p95_latency > 500ms
  # Users care vá» latency, khÃ´ng pháº£i CPU
  # Root cause cÃ³ thá»ƒ lÃ  CPU, memory, network, database...
```

**Workflow:**

```
1. Symptom alert fires: "SlowRequests"
   â†’ On-call investigates
   
2. Check potential causes:
   - CPU high? â†’ Scale up
   - Memory high? â†’ Fix leak
   - Database slow? â†’ Optimize queries
   - Network issue? â†’ Check connectivity
   
3. Fix root cause
   â†’ Symptom resolves
```

**Benefit:**
- Focus on user impact
- Avoid false positives (high CPU nhÆ°ng latency OK)
- Flexible (nhiá»u causes cÃ³ thá»ƒ gÃ¢y cÃ¹ng symptom)

---

## 4. Dashboard Design - Thá»±c táº¿

### Hierarchy Approach

**Level 1: Overview Dashboard (Glance trong 5 giÃ¢y)**

```
Purpose: "Há»‡ thá»‘ng cÃ³ OK khÃ´ng?"

Panels:
  1. Traffic (requests/s) - Trending up/down?
  2. Error rate (%) - Trong threshold?
  3. Latency (p95) - Users happy?
  4. Saturation (CPU, memory, disk) - Need to scale?
  
Color coding:
  Green: All good
  Yellow: Warning
  Red: Critical

Example:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ System Health: âœ… HEALTHY           â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ Traffic:      1,234 req/s  âœ…       â”‚
  â”‚ Error Rate:   0.5%         âœ…       â”‚
  â”‚ Latency p95:  150ms        âœ…       â”‚
  â”‚ CPU:          65%          âœ…       â”‚
  â”‚ Memory:       72%          âœ…       â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Level 2: Service Dashboard (Drill down khi cÃ³ váº¥n Ä‘á»)**

```
Purpose: "Service nÃ o cÃ³ váº¥n Ä‘á»?"

Panels per service:
  1. Request rate by endpoint
  2. Error rate by endpoint
  3. Latency distribution (p50, p95, p99)
  4. Dependencies status
  5. Resource usage

Example:
  API Service Dashboard
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ /api/users:     500 req/s  âœ…       â”‚
  â”‚ /api/orders:    300 req/s  âš ï¸ slow â”‚
  â”‚ /api/products:  200 req/s  âœ…       â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ Dependencies:                        â”‚
  â”‚   PostgreSQL:   âœ… Healthy          â”‚
  â”‚   Redis:        âš ï¸ High latency     â”‚
  â”‚   Payment API:  âœ… Healthy          â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Level 3: Deep Dive (Debug specific issue)**

```
Purpose: "Táº¡i sao /api/orders cháº­m?"

Panels:
  1. Query breakdown (which queries slow?)
  2. Database connection pool
  3. Cache hit ratio
  4. External API calls
  5. Slow query logs

Example:
  /api/orders Deep Dive
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Slow queries:                        â”‚
  â”‚   SELECT * FROM orders WHERE...     â”‚
  â”‚   Duration: 2.5s âŒ                 â”‚
  â”‚   Execution count: 1,234/min        â”‚
  â”‚                                      â”‚
  â”‚ Root cause: Missing index on        â”‚
  â”‚   orders.user_id                    â”‚
  â”‚                                      â”‚
  â”‚ Action: CREATE INDEX idx_user_id    â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Dashboard Anti-Patterns

**âŒ Too Many Panels:**

```
Dashboard vá»›i 50 panels:
  - Overwhelming
  - KhÃ´ng biáº¿t nhÃ¬n cÃ¡i nÃ o
  - Slow to load
  
Better: < 10 panels per dashboard
```

**âŒ No Context:**

```
Panel: "CPU Usage"
  - CPU cá»§a cÃ¡i gÃ¬?
  - Threshold lÃ  bao nhiÃªu?
  - High CPU cÃ³ OK khÃ´ng?
  
Better: "API Server CPU (target < 70%)"
```

**âŒ No Time Context:**

```
Panel chá»‰ show current value:
  CPU: 85%
  
  - Äang tÄƒng hay giáº£m?
  - Spike hay sustained?
  - Compare vá»›i yesterday?
  
Better: Show trend (last 24h, last 7d)
```

---

## 5. Observability Maturity - Journey

### Level 1: Reactive (Firefighting)

**Characteristics:**
```
- Biáº¿t cÃ³ váº¥n Ä‘á» khi users complain
- KhÃ´ng cÃ³ metrics
- Logs scattered, khÃ´ng structured
- Debug báº±ng SSH vÃ o server
- "Works on my machine"
```

**Example incident:**

```
10:00 AM: Users report "website down"
10:05 AM: Team starts investigating
10:30 AM: Still don't know what's wrong
11:00 AM: Try restarting everything
11:30 AM: Still down
12:00 PM: Call vendor support
14:00 PM: Finally fixed (database ran out of disk)

MTTD: 5 minutes (user reported)
MTTR: 4 hours
Total downtime: 4 hours
```

**Cost:**
- Lost revenue: 4 hours Ã— $10,000/hour = $40,000
- Customer trust: Priceless
- Team stress: High

---

### Level 2: Proactive (Basic Monitoring)

**Characteristics:**
```
- Basic metrics (CPU, memory, disk)
- Simple alerts (disk > 90%)
- Logs collected centrally
- Some dashboards
```

**Same incident with Level 2:**

```
09:45 AM: Alert fires "Disk > 90%"
09:50 AM: On-call checks, sees database disk full
09:55 AM: Cleanup old logs
10:00 AM: Back to normal

MTTD: 15 minutes (before users notice)
MTTR: 15 minutes
Total downtime: 0 (prevented)
```

**Improvement:**
- Prevented outage
- Saved $40,000
- Happy customers
- Less stress

---

### Level 3: Predictive (Advanced Monitoring)

**Characteristics:**
```
- Comprehensive metrics (Golden Signals)
- SLI/SLO tracking
- Structured logs vá»›i correlation
- Distributed tracing
- Capacity planning
```

**Same scenario with Level 3:**

```
Monday: Dashboard shows disk usage trending
  - Current: 70%
  - Growth rate: 5%/day
  - Prediction: 90% in 4 days (Friday)

Tuesday: Create ticket "Increase disk or cleanup"
Wednesday: Implement log rotation
Thursday: Disk usage drops to 60%
Friday: No incident

MTTD: 4 days before problem
MTTR: N/A (prevented)
Total downtime: 0
```

**Improvement:**
- Predicted problem before it happened
- Planned fix during business hours
- No stress, no rush
- Proactive, not reactive

---

### Level 4: Autonomous (Self-Healing)

**Characteristics:**
```
- Auto-scaling
- Auto-remediation
- Anomaly detection (ML)
- Chaos engineering
- Full automation
```

**Same scenario with Level 4:**

```
System detects disk growth pattern
â†’ Auto-triggers cleanup job
â†’ Removes old logs
â†’ Disk usage stays at 60%
â†’ No human intervention

MTTD: Real-time
MTTR: Automatic
Total downtime: 0
Human effort: 0
```

---

## 6. Practical Implementation Guide

### Step 1: Start with Golden Signals

**Week 1: Implement Latency tracking**

```promql
# Add to your application
histogram_quantile(0.95, 
  rate(http_request_duration_seconds_bucket[5m])
)

# Create dashboard panel
# Create alert if p95 > 500ms
```

**Week 2: Add Traffic monitoring**

```promql
rate(http_requests_total[5m])

# Create dashboard
# Create alert for sudden changes
```

**Week 3: Track Errors**

```promql
rate(http_requests_total{status=~"5.."}[5m])
/
rate(http_requests_total[5m])

# Alert if > 1%
```

**Week 4: Monitor Saturation**

```promql
# CPU, memory, disk, connections
# Alert at 80%, not 95%
```

### Step 2: Define SLO

**Month 2: Baseline**

```
Collect data for 30 days:
  - What's current availability?
  - What's current latency?
  - What's acceptable?
```

**Month 3: Set SLO**

```yaml
SLO:
  - Availability: 99.9%
  - Latency: p95 < 200ms
  - Error rate: < 1%
```

**Month 4: Track Error Budget**

```
Monitor monthly:
  - Error budget used
  - Error budget remaining
  - Adjust velocity accordingly
```

### Step 3: Improve Alerts

**Month 5: Audit Alerts**

```
For each alert:
  - Is it actionable?
  - Is it symptom-based?
  - Does it have runbook?
  - Is severity correct?
  
Remove/fix bad alerts
```

**Month 6: Add Context**

```yaml
# Update all alerts with:
annotations:
  summary: Clear description
  impact: User impact
  runbook: Link to runbook
  dashboard: Link to dashboard
```

### Step 4: Build Dashboards

**Month 7: Overview Dashboard**

```
Single dashboard showing:
  - Golden Signals
  - SLO status
  - Error budget
  - Top issues
```

**Month 8: Service Dashboards**

```
One dashboard per service:
  - Service-specific metrics
  - Dependencies
  - Resource usage
```

### Step 5: Continuous Improvement

**Ongoing:**

```
Weekly:
  - Review alerts (any false positives?)
  - Check SLO status
  - Update runbooks

Monthly:
  - Review error budget
  - Capacity planning
  - Update SLO if needed

Quarterly:
  - Post-mortems review
  - Process improvements
  - Training
```

---

## 7. Common Pitfalls & Solutions

### Pitfall 1: "We monitor everything!"

**Problem:**
```
1000 metrics collected
100 dashboards created
500 alerts configured

Result:
  - Nobody knows what to look at
  - Alert fatigue
  - High costs
  - Low value
```

**Solution:**
```
Start with Golden Signals (4 metrics)
Add more only when needed
Remove unused metrics/dashboards/alerts
```

### Pitfall 2: "Average is good enough"

**Problem:**
```
Average latency: 100ms âœ…

Reality:
  - 90% requests: 50ms
  - 9% requests: 200ms
  - 1% requests: 5000ms (timeout)

Users complain but metrics look good
```

**Solution:**
```
Always use percentiles (p50, p95, p99)
Never rely on average alone
```

### Pitfall 3: "Alert on everything"

**Problem:**
```
Alert when:
  - CPU > 50%
  - Memory > 60%
  - Disk > 70%
  - Any error occurs
  - Traffic changes

Result: 100 alerts/day, all ignored
```

**Solution:**
```
Alert only on:
  - User-impacting symptoms
  - SLO violations
  - Imminent disasters (disk 90%)

Target: < 5 alerts/day
```

### Pitfall 4: "No runbooks"

**Problem:**
```
Alert fires at 3 AM
On-call wakes up
Doesn't know what to do
Escalates to senior
Senior also doesn't remember
Spend 2 hours figuring out
```

**Solution:**
```
Every alert must have runbook:
  1. What's happening?
  2. Why does it matter?
  3. How to investigate?
  4. How to fix?
  5. Who to escalate to?
```

---

## 8. Success Metrics

### How to measure if your monitoring is good?

**1. MTTD (Mean Time To Detect)**

```
Target: < 5 minutes

Measure:
  - Time from incident start to alert
  - Track monthly average
  - Trend should go down
```

**2. MTTR (Mean Time To Resolve)**

```
Target: < 30 minutes (critical)

Measure:
  - Time from alert to resolution
  - Track per incident type
  - Identify patterns
```

**3. Alert Quality**

```
Precision = True Positives / (True Positives + False Positives)
Target: > 90%

Track:
  - How many alerts were real issues?
  - How many were false alarms?
```

**4. SLO Compliance**

```
Target: Meet SLO 95% of months

Track:
  - Monthly SLO achievement
  - Error budget usage
  - Trend over time
```

**5. Incident Reduction**

```
Target: 50% reduction year-over-year

Track:
  - Number of incidents/month
  - Severity distribution
  - Repeat incidents (should decrease)
```

---

## ğŸ¯ Summary

**Key Principles:**

1. **Start Simple**: Golden Signals first, complexity later
2. **User Focus**: Monitor symptoms, not just causes
3. **Data-Driven**: SLO/error budget guide decisions
4. **Actionable**: Every alert needs clear action
5. **Continuous**: Always improving, never done

**Your Next Steps:**

1. âœ… Implement Golden Signals (Week 1-4)
2. âœ… Define SLO (Month 2-3)
3. âœ… Improve alerts (Month 5-6)
4. âœ… Build dashboards (Month 7-8)
5. âœ… Continuous improvement (Ongoing)

**Remember:**
- Perfect is enemy of good
- Start small, iterate
- Measure, learn, improve
- Focus on user impact

Good luck! ğŸš€
