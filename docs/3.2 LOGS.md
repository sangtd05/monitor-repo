# LOGS - H·ªá th·ªëng Thu th·∫≠p v√† Ph√¢n t√≠ch Logs

## üéØ Logs l√† g√¨ v√† T·∫°i sao c·∫ßn Logs?

### ƒê·ªãnh nghƒ©a

**Logs** l√† c√°c b·∫£n ghi (records) v·ªÅ c√°c s·ª± ki·ªán (events) x·∫£y ra trong h·ªá th·ªëng theo th·ªùi gian. M·ªói log entry th∆∞·ªùng ch·ª©a timestamp, level, message, v√† context.

```
2025-12-30 15:30:45 INFO  User login successful user_id=123 ip=10.0.0.1
2025-12-30 15:30:46 ERROR Database connection failed error="timeout after 5s"
2025-12-30 15:30:47 WARN  Cache miss key="user:123" fallback="database"
```

### So s√°nh v·ªõi Metrics v√† Traces

| Aspect | Logs | Metrics | Traces |
|--------|------|---------|--------|
| **D·ªØ li·ªáu** | Events r·ªùi r·∫°c | S·ªë ƒëo theo th·ªùi gian | Request journey |
| **Cardinality** | Cao (m·ªói event kh√°c nhau) | Th·∫•p (aggregated) | Trung b√¨nh |
| **Storage** | Nhi·ªÅu | √çt | Trung b√¨nh |
| **Use case** | Debugging, audit | Monitoring, alerting | Performance analysis |
| **C√¢u h·ªèi** | "Chuy·ªán g√¨ ƒë√£ x·∫£y ra?" | "H·ªá th·ªëng th·∫ø n√†o?" | "Request ƒëi ƒë√¢u?" |

### T·∫°i sao Logs quan tr·ªçng?

#### 1. **Debugging v√† Troubleshooting**

**Scenario**: API tr·∫£ v·ªÅ l·ªói 500.

**Metrics ch·ªâ cho bi·∫øt:**
```promql
http_requests_total{status="500"} = 10
```
‚Üí Bi·∫øt c√≥ 10 requests l·ªói, nh∆∞ng kh√¥ng bi·∫øt t·∫°i sao.

**Logs cho bi·∫øt:**
```
2025-12-30 15:30:45 ERROR [OrderController] Failed to create order
  user_id: 12345
  order_id: abc-123
  error: "Database connection timeout after 5s"
  stack_trace: |
    at OrderService.create (order.service.js:45)
    at OrderController.createOrder (order.controller.js:23)
  context: {
    "db_host": "postgres-primary",
    "connection_pool_size": 0,
    "waiting_connections": 50
  }
```
‚Üí Bi·∫øt ch√≠nh x√°c: connection pool h·∫øt, database timeout.

#### 2. **Audit Trail**

Theo d√µi ai l√†m g√¨, khi n√†o:
```
2025-12-30 15:30:45 AUDIT User admin@company.com deleted user user_id=999
2025-12-30 15:31:00 AUDIT User john@company.com updated permissions role=admin
2025-12-30 15:32:15 AUDIT Failed login attempt username=hacker ip=1.2.3.4
```

**Use cases:**
- Security investigations
- Compliance (GDPR, SOC2)
- Dispute resolution

#### 3. **Business Intelligence**

Ph√¢n t√≠ch h√†nh vi ng∆∞·ªùi d√πng:
```
2025-12-30 15:30:45 INFO User viewed product product_id=123 category=electronics
2025-12-30 15:30:50 INFO User added to cart product_id=123 quantity=1
2025-12-30 15:31:00 INFO User completed checkout order_id=abc-123 total=99.99
```

**Insights:**
- Conversion funnel
- Popular products
- User journey analysis

#### 4. **Error Tracking**

T√¨m patterns trong errors:
```logql
{service="api"} |= "error" | json | line_format "{{.error_type}}"
```

**K·∫øt qu·∫£:**
```
DatabaseTimeout: 45%
ValidationError: 30%
ExternalAPIError: 15%
UnknownError: 10%
```
‚Üí ∆Øu ti√™n fix DatabaseTimeout tr∆∞·ªõc.

## üèóÔ∏è Ki·∫øn tr√∫c Logging trong H·ªá th·ªëng

```mermaid
graph TB
    classDef source fill:#e1f5ff,stroke:#0277bd,stroke-width:2px,rx:5,ry:5
    classDef alloy fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,rx:5,ry:5
    classDef component fill:#fff3e0,stroke:#ef6c00,stroke-width:2px,rx:3,ry:3
    classDef storage fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,rx:5,ry:5

    subgraph "Log Sources"
        DOCKER[Docker Containers<br/>stdout/stderr]:::source
        SYS[System Logs<br/>/var/log/*]:::source
        EXP[Exporters<br/>Logs]:::source
    end
    
    subgraph "Grafana Alloy (Unified Agent)"
        DISCO[Service Discovery<br/>discovery.docker]:::component
        RELABEL[Relabeling<br/>discovery.relabel]:::component
        LOKI_SRC[Loki Source<br/>loki.source.docker<br/>loki.source.file]:::component
        PROCESS[Processing<br/>loki.process<br/>(Filter & Clean)]:::component
        WRITE[Loki Write<br/>loki.write]:::component
    end
    
    subgraph "Storage"
        LOKI[Loki<br/>Log Aggregation]:::storage
    end
    
    DOCKER --> DISCO
    DISCO --> RELABEL
    RELABEL --> LOKI_SRC
    SYS --> LOKI_SRC
    
    LOKI_SRC --> PROCESS
    PROCESS --> WRITE
    WRITE --> LOKI
    
    style DISCO fill:#fff3e0
    style RELABEL fill:#fff3e0
    style LOKI_SRC fill:#fff3e0
    style PROCESS fill:#fff3e0
    style WRITE fill:#fff3e0
```

## üîß C·∫•u h√¨nh Grafana Alloy cho Logs

**Grafana Alloy** thay th·∫ø ho√†n to√†n Promtail trong vi·ªác thu th·∫≠p logs. C·∫•u h√¨nh ƒë∆∞·ª£c vi·∫øt b·∫±ng **Alloy syntax** (HCL-like), n·∫±m trong file `alloy/config.alloy`.

### 1. Concepts c∆° b·∫£n

Trong Alloy, data flow ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a qua c√°c **components** n·ªëi v·ªõi nhau:
- **`discovery.*`**: T√¨m ki·∫øm targets (v√≠ d·ª•: t√¨m docker containers).
- **`loki.source.*`**: ƒê·ªçc logs t·ª´ targets (file, docker socket, syslog).
- **`loki.process`**: X·ª≠ l√Ω logs (drop, parsing, label extraction).
- **`loki.write`**: G·ª≠i logs ƒë·∫øn Loki.

### 2. Thu th·∫≠p Docker Logs

Thay v√¨ d√πng static config nh∆∞ Promtail, Alloy s·ª≠ d·ª•ng dynamic discovery m·∫°nh m·∫Ω h∆°n.

```alloy
// 1. T√¨m ki·∫øm Docker containers ƒëang ch·∫°y
discovery.docker "containers" {
  host = "unix:///var/run/docker.sock"
  refresh_interval = "5s"
}

// 2. L·ªçc v√† g√°n labels (Relabeling)
discovery.relabel "docker_logs" {
  targets = discovery.docker.containers.targets

  // Drop logs t·ª´ ch√≠nh Alloy (tr√°nh loop)
  rule {
    source_labels = ["__meta_docker_container_name"]
    regex         = ".*(loki|alloy).*"
    action        = "drop"
  }

  // G√°n label 'service' t·ª´ docker-compose service name
  rule {
    source_labels = ["__meta_docker_container_label_com_docker_compose_service"]
    target_label  = "service"
  }
}

// 3. ƒê·ªçc logs t·ª´ list containers ƒë√£ l·ªçc
loki.source.docker "containers" {
  host             = "unix:///var/run/docker.sock"
  targets          = discovery.relabel.docker_logs.output
  forward_to       = [loki.process.docker_logs.receiver] // G·ª≠i ƒë·∫øn processor
}
```

### 3. X·ª≠ l√Ω Logs (Processing)

```alloy
loki.process "docker_logs" {
  // Drop corrupted logs (null bytes)
  stage.drop {
    expression = ".*[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F].*"
    drop_counter_reason = "corrupted_log"
  }

  // Drop logs c≈© h∆°n 24h
  stage.drop {
    older_than = "24h"
    drop_counter_reason = "log_too_old"
  }

  forward_to = [loki.write.default.receiver] // G·ª≠i ƒë·∫øn writer
}
```

### 4. G·ª≠i ƒë·∫øn Loki (Write)

```alloy
loki.write "default" {
  endpoint {
    url = "http://loki:3100/loki/api/v1/push"
    tenant_id = "1"
  }
}
```

### 5. Thu th·∫≠p System Logs (File Scraping)

ƒê·ªÉ ƒë·ªçc system logs (`/var/log/*.log`):

```alloy
// ƒê·ªãnh nghƒ©a file pattern
local.file_match "system_logs" {
  path_targets = [{
    __address__ = "localhost",
    __path__     = "/var/log/*.log",
    job         = "varlogs",
  }]
}

// ƒê·ªçc file
loki.source.file "system" {
  targets    = local.file_match.system_logs.targets
  forward_to = [loki.process.system_logs.receiver]
}
```

### So s√°nh Promtail vs Alloy

| Feature | Promtail (YAML) | Alloy (Block Syntax) |
|---------|-----------------|----------------------|
| **Format** | YAML List/Dict | Blocks & Pipelines (r·∫•t d·ªÖ ƒë·ªçc) |
| **Flow** | Pipeline Stages | Components n·ªëi v·ªõi nhau (forward_to) |
| **Logic** | Tuy·∫øn t√≠nh | Graph-based (c√≥ th·ªÉ r·∫Ω nh√°nh, g·ªôp d√≤ng) |
| **Debug** | Kh√≥ | UI tr·ª±c quan (port 12345) xem data flow live |


### 2. Loki - Log Aggregation System

**Loki** l√† log aggregation system c·ªßa Grafana Labs, ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ "like Prometheus, but for logs".

#### Tri·∫øt l√Ω Thi·∫øt k·∫ø

**Prometheus approach:**
```
Metric: http_requests_total{method="GET", endpoint="/api/users", status="200"}
        ‚îÇ                   ‚îÇ
        ‚îÇ                   ‚îî‚îÄ Labels (indexed)
        ‚îî‚îÄ Metric name + value (not indexed)
```

**Loki approach:**
```
Log: "2025-12-30 15:30:45 GET /api/users 200 50ms"
     ‚îÇ                                              ‚îÇ
     ‚îÇ                                              ‚îî‚îÄ Log content (not indexed)
     ‚îî‚îÄ Labels: {job="api", level="info"} (indexed)
```

**Key difference:**
- **Prometheus**: Index everything (metric name + labels)
- **Loki**: Ch·ªâ index labels, kh√¥ng index log content
- **ELK**: Index to√†n b·ªô log content (full-text search)

**Trade-offs:**

| Feature | Loki | Elasticsearch |
|---------|------|---------------|
| **Index size** | Nh·ªè (ch·ªâ labels) | L·ªõn (full content) |
| **Storage cost** | Th·∫•p | Cao |
| **Query speed** | Nhanh (label filtering) | Nhanh (full-text) |
| **Full-text search** | ‚ùå (grep-like) | ‚úÖ (powerful) |
| **Setup complexity** | ƒê∆°n gi·∫£n | Ph·ª©c t·∫°p |
| **Resource usage** | Th·∫•p | Cao |

**Khi n√†o d√πng Loki?**
- ‚úÖ Label-based filtering ƒë·ªß d√πng
- ‚úÖ ƒê√£ d√πng Prometheus (consistent UX)
- ‚úÖ Cost-sensitive
- ‚ùå C·∫ßn full-text search ph·ª©c t·∫°p
- ‚ùå C·∫ßn search trong log content

#### C·∫•u h√¨nh Loki

```yaml
auth_enabled: false
```
**Multi-tenancy:**
- `true`: C·∫ßn `X-Scope-OrgID` header
- `false`: Single tenant mode (ƒë∆°n gi·∫£n h∆°n)

```yaml
server:
  http_listen_port: 3100
  grpc_listen_port: 9096
  log_level: info
```

**Ports:**
- 3100: HTTP API (push logs, query)
- 9096: gRPC (internal communication)

```yaml
common:
  path_prefix: /loki
  storage:
    filesystem:
      chunks_directory: /loki/chunks
      rules_directory: /loki/rules
  replication_factor: 1
```

**Storage structure:**
```
/loki/
‚îú‚îÄ‚îÄ chunks/          ‚Üê Log data (compressed)
‚îú‚îÄ‚îÄ rules/           ‚Üê Alert rules
‚îî‚îÄ‚îÄ compactor/       ‚Üê Compaction working dir
```

**Replication factor:**
- `1`: Single instance (no replication)
- `3`: Production (HA setup)

#### Schema Config

```yaml
schema_config:
  configs:
    - from: 2024-01-01
      store: tsdb
      object_store: filesystem
      schema: v13
      index:
        prefix: index_
        period: 24h
```

**Gi·∫£i th√≠ch:**

**1. Store: TSDB (Time Series Database)**
- Loki v2.8+ s·ª≠ d·ª•ng TSDB index
- T∆∞∆°ng t·ª± Prometheus TSDB
- T·ªët h∆°n BoltDB (legacy)

**2. Schema v13:**
- Latest schema version
- T·ªëi ∆∞u cho TSDB

**3. Index period: 24h**
- T·∫°o index m·ªõi m·ªói ng√†y
- File: `index_2025-12-30`
- Trade-off:
  - 24h: Balance gi·ªØa file size v√† query performance
  - 12h: Nhi·ªÅu files, query ph·ª©c t·∫°p h∆°n
  - 168h (7d): √çt files, nh∆∞ng file l·ªõn

#### Compactor - N√©n v√† Retention

```yaml
compactor:
  working_directory: /loki/compactor
  compaction_interval: 10m
  retention_enabled: true
  retention_delete_delay: 2h
  retention_delete_worker_count: 150
  delete_request_store: filesystem
```

**Compaction workflow:**

```
Ingester t·∫°o chunks:
chunk-001 (0-10min, 10MB)
chunk-002 (10-20min, 12MB)
chunk-003 (20-30min, 8MB)
...

Compactor (m·ªói 10 ph√∫t):
1. Merge chunks: chunk-001 + chunk-002 + chunk-003 ‚Üí chunk-hour-1 (25MB)
2. Compress: chunk-hour-1 (25MB) ‚Üí chunk-hour-1.gz (5MB)
3. Delete old chunks
```

**L·ª£i √≠ch:**
- **Storage**: 10MB + 12MB + 8MB = 30MB ‚Üí 5MB (83% savings)
- **Query**: ƒê·ªçc 1 file thay v√¨ 3 files
- **I/O**: √çt file operations

**Retention:**
```yaml
limits_config:
  retention_period: 168h  # 7 days
```

**Deletion workflow:**
```
Day 1: Ingest logs
Day 2-7: Keep logs
Day 8: Compactor marks for deletion
Day 8 + 2h: Actually delete (retention_delete_delay)
```

**T·∫°i sao delay 2h?**
- Queries ƒëang ch·∫°y c√≥ th·ªÉ ƒë·ªçc chunks
- Delay ƒë·∫£m b·∫£o queries ho√†n th√†nh
- Tr√°nh "file not found" errors

#### Limits Config

```yaml
limits_config:
  retention_period: 168h
  reject_old_samples: false
  reject_old_samples_max_age: 8760h  # 1 year
  creation_grace_period: 8760h
  ingestion_rate_mb: 50
  ingestion_burst_size_mb: 100
  max_streams_per_user: 0  # unlimited
  max_line_size: 256KB
  split_queries_by_interval: 15m
```

**Gi·∫£i th√≠ch t·ª´ng config:**

**1. reject_old_samples: false**
```
Scenario: Promtail restart, ƒë·ªçc l·∫°i logs t·ª´ 2 ng√†y tr∆∞·ªõc

reject_old_samples: true
  ‚Üí Loki reject: "Log too old"
  ‚Üí M·∫•t logs

reject_old_samples: false
  ‚Üí Loki accept
  ‚Üí C√≥ th·ªÉ c√≥ duplicates, nh∆∞ng kh√¥ng m·∫•t data
```

**2. max_age: 8760h (1 year)**
- Ch·∫•p nh·∫≠n logs t·ªëi ƒëa 1 nƒÉm tu·ªïi
- Tr√°nh spam t·ª´ corrupted timestamps

**3. ingestion_rate_mb: 50**
```
Normal: 10MB/s ‚Üí OK
Spike: 60MB/s ‚Üí Throttled (429 error)
```

**T·∫°i sao throttle?**
- B·∫£o v·ªá Loki kh·ªèi overload
- Promtail retry sau

**4. ingestion_burst_size_mb: 100**
```
Burst: Cho ph√©p v∆∞·ª£t rate limit trong th·ªùi gian ng·∫Øn
Example:
  - 0-1s: 100MB (burst)
  - 1-2s: 50MB (normal rate)
  - 2-3s: 50MB
```

**5. max_streams_per_user: 0 (unlimited)**

**Stream** = unique label combination:
```
{job="api", level="info"}     ‚Üí stream 1
{job="api", level="error"}    ‚Üí stream 2
{job="db", level="info"}      ‚Üí stream 3
```

**V·∫•n ƒë·ªÅ high cardinality:**
```yaml
# BAD: user_id in labels
{job="api", user_id="123"}
{job="api", user_id="456"}
{job="api", user_id="789"}
...
‚Üí 1 million users = 1 million streams ‚Üí OOM
```

**Good practice:**
```yaml
# GOOD: user_id in log content
{job="api", level="info"}
log: "User 123 logged in"
```

**6. split_queries_by_interval: 15m**

**Query:**
```logql
{job="api"} | json | line_format "{{.message}}"
  [last 24 hours]
```

**Without split:**
```
1 query: Scan 24 hours of data
‚Üí Timeout, OOM
```

**With split (15m intervals):**
```
Query 1: 00:00-00:15
Query 2: 00:15-00:30
...
Query 96: 23:45-24:00

‚Üí Parallel execution
‚Üí Faster, more reliable
```

#### Ruler - Alerting

```yaml
ruler:
  alertmanager_url: http://alertmanager:9093
```

**Loki alerting:**
```yaml
# loki-rules.yml
groups:
  - name: errors
    interval: 1m
    rules:
      - alert: HighErrorRate
        expr: |
          sum(rate({job="api"} |= "error" [5m])) > 10
        for: 5m
        annotations:
          summary: "High error rate in API"
```

**Workflow:**
```
Loki Ruler (m·ªói 1 ph√∫t)
  ‚Üí Run LogQL query
  ‚Üí If condition true for 5m
  ‚Üí Send alert to Alertmanager
  ‚Üí Alertmanager route to Telegram
```

### 3. LogQL - Query Language

**LogQL** gi·ªëng PromQL, nh∆∞ng cho logs.

#### Log Stream Selector

```logql
# Basic
{job="api"}

# Multiple labels
{job="api", level="error"}

# Regex
{job=~"api|web"}
{job!~"test.*"}
```

#### Line Filter

```logql
# Contains
{job="api"} |= "error"

# Not contains
{job="api"} != "debug"

# Regex
{job="api"} |~ "error|exception|failed"

# Case-insensitive
{job="api"} |~ "(?i)error"
```

#### Parser

**JSON:**
```logql
{job="api"} | json
```

**Input:**
```json
{"level":"error","message":"DB timeout","user_id":123}
```

**Output:** Extract fields as labels
```
level="error"
message="DB timeout"
user_id="123"
```

**Logfmt:**
```logql
{job="api"} | logfmt
```

**Input:**
```
level=error message="DB timeout" user_id=123
```

**Regex:**
```logql
{job="api"} | regexp "(?P<method>\\w+) (?P<path>/\\S+) (?P<status>\\d+)"
```

**Input:**
```
GET /api/users 200
```

**Output:**
```
method="GET"
path="/api/users"
status="200"
```

#### Label Filter (after parsing)

```logql
{job="api"} 
  | json 
  | level="error"
  | status_code >= 500
```

#### Line Format

```logql
{job="api"} 
  | json 
  | line_format "{{.timestamp}} [{{.level}}] {{.message}}"
```

**Input:**
```json
{"timestamp":"2025-12-30T15:30:45Z","level":"ERROR","message":"Timeout"}
```

**Output:**
```
2025-12-30T15:30:45Z [ERROR] Timeout
```

#### Aggregation

```logql
# Count
count_over_time({job="api"} |= "error" [5m])

# Rate (logs per second)
rate({job="api"} [5m])

# Sum extracted values
sum(rate({job="api"} | json | unwrap bytes [5m]))

# Quantile
quantile_over_time(0.95, {job="api"} | json | unwrap duration [5m])
```

#### Useful Queries

**Error rate:**
```logql
sum(rate({job="api"} |= "error" [5m])) by (service)
```

**Top error messages:**
```logql
topk(10, 
  sum by (message) (
    count_over_time({job="api", level="error"} | json [1h])
  )
)
```

**Slow queries:**
```logql
{job="postgres"} 
  | regexp "duration: (?P<duration>\\d+\\.\\d+) ms"
  | duration > 1000
```

**Failed logins:**
```logql
{job="auth"} 
  |= "login failed"
  | json
  | line_format "{{.timestamp}} {{.username}} {{.ip}}"
```

## üí° Best Practices

### 1. Label Design

**Good (low cardinality):**
```
{job="api", env="prod", level="error"}
```

**Bad (high cardinality):**
```
{job="api", user_id="123", request_id="abc-def"}
```

**Rule:** Labels n√™n c√≥ < 100 unique values.

### 2. Structured Logging

**Good (JSON):**
```json
{"timestamp":"2025-12-30T15:30:45Z","level":"ERROR","message":"DB timeout","user_id":123,"duration_ms":5000}
```

**Bad (unstructured):**
```
[2025-12-30 15:30:45] ERROR: Database timeout for user 123 after 5000ms
```

**L·ª£i √≠ch structured:**
- D·ªÖ parse v·ªõi `| json`
- Extract fields th√†nh labels
- Aggregate, filter ch√≠nh x√°c

### 3. Log Levels

```
TRACE: Very detailed (disabled in prod)
DEBUG: Detailed info for debugging
INFO: General informational messages
WARN: Warning, potential issues
ERROR: Errors, but app still running
FATAL: Critical errors, app crash
```

**Best practice:**
- Production: INFO v√† cao h∆°n
- Development: DEBUG
- Troubleshooting: TRACE (t·∫°m th·ªùi)

### 4. Context in Logs

**Good:**
```json
{
  "level": "error",
  "message": "Payment failed",
  "user_id": 123,
  "order_id": "abc-123",
  "payment_method": "credit_card",
  "amount": 99.99,
  "error_code": "CARD_DECLINED",
  "trace_id": "xyz-789"
}
```

**Bad:**
```
ERROR: Payment failed
```

**Include:**
- User/session context
- Request context
- Error details
- Trace ID (link to distributed tracing)

## üéì T·ªïng k·∫øt

### Logging Flow Summary

1. **Applications** ghi logs (stdout/stderr/files)
2. **Grafana Alloy** thu th·∫≠p logs t·ª´ Docker, files, system
3. **Alloy components** parse, relabel, filter logs
4. **Loki** nh·∫≠n logs, index labels, store chunks
5. **Compactor** n√©n v√† x√≥a logs c≈©
6. **Grafana** query logs v·ªõi LogQL, visualize

### Key Takeaways

‚úÖ **Logs = Event records v·ªõi timestamp v√† context**
‚úÖ **Loki = Label-based indexing (like Prometheus)**
‚úÖ **Grafana Alloy = Unified Observability Agent (thay th·∫ø Promtail)**
‚úÖ **LogQL = Query language gi·ªëng PromQL**
‚úÖ **Structured logging = D·ªÖ parse v√† analyze**
‚úÖ **Low cardinality labels = Hi·ªáu qu·∫£ v√† scalable**

### Khi n√†o d√πng Logs?

- ‚úÖ Debugging chi ti·∫øt (stack traces, errors)
- ‚úÖ Audit trail (who did what when)
- ‚úÖ Security investigations
- ‚úÖ Business analytics (user behavior)
- ‚úÖ Compliance (GDPR, SOC2)
- ‚ùå System-wide trends (d√πng Metrics)
- ‚ùå Request tracing (d√πng Traces)
- ‚ùå Real-time alerting (d√πng Metrics, nhanh h∆°n)

### So s√°nh 3 Pillars

| Use Case | Logs | Metrics | Traces |
|----------|------|---------|--------|
| "T·∫°i sao API ch·∫≠m?" | ‚ùå | ‚ö†Ô∏è Bi·∫øt ch·∫≠m | ‚úÖ Bi·∫øt ch·∫≠m ·ªü ƒë√¢u |
| "C√≥ bao nhi√™u errors?" | ‚ö†Ô∏è Count | ‚úÖ Counter | ‚ùå |
| "Error message l√† g√¨?" | ‚úÖ | ‚ùå | ‚ö†Ô∏è Span events |
| "Ai x√≥a user n√†y?" | ‚úÖ Audit log | ‚ùå | ‚ùå |
| "CPU usage trend?" | ‚ùå | ‚úÖ | ‚ùå |
| "Request ƒëi qua services n√†o?" | ‚ùå | ‚ùå | ‚úÖ |

**Best practice:** D√πng c·∫£ 3 k·∫øt h·ª£p!
- **Metrics**: Ph√°t hi·ªán v·∫•n ƒë·ªÅ
- **Traces**: T√¨m bottleneck
- **Logs**: Debug chi ti·∫øt
