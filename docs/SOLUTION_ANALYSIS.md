# PH√ÇN T√çCH V·∫§N ƒê·ªÄ V√Ä L·ª∞A CH·ªåN GI·∫¢I PH√ÅP

> **T√†i li·ªáu n√†y ph√¢n t√≠ch v·∫•n ƒë·ªÅ monitoring, so s√°nh c√°c gi·∫£i ph√°p, v√† gi·∫£i th√≠ch l√Ω do l·ª±a ch·ªçn LGTM Stack**

## üìã M·ª•c l·ª•c

- [1. B·ªëi c·∫£nh v√† V·∫•n ƒë·ªÅ](#1-b·ªëi-c·∫£nh-v√†-v·∫•n-ƒë·ªÅ)
- [2. Y√™u c·∫ßu H·ªá th·ªëng Monitoring](#2-y√™u-c·∫ßu-h·ªá-th·ªëng-monitoring)
- [3. Ph√¢n lo·∫°i ƒê·ªëi t∆∞·ª£ng](#3-ph√¢n-lo·∫°i-ƒë·ªëi-t∆∞·ª£ng)
- [4. So s√°nh Gi·∫£i ph√°p](#4-so-s√°nh-gi·∫£i-ph√°p)
- [5. Quy·∫øt ƒë·ªãnh L·ª±a ch·ªçn](#5-quy·∫øt-ƒë·ªãnh-l·ª±a-ch·ªçn)
- [6. Roadmap Tri·ªÉn khai](#6-roadmap-tri·ªÉn-khai)

---

## 1. B·ªëi c·∫£nh v√† V·∫•n ƒë·ªÅ

### 1.1. B·ªëi c·∫£nh D·ª± √°n

**Quy m√¥ h·∫° t·∫ßng hi·ªán t·∫°i**:
- **31+ servers** ƒëang v·∫≠n h√†nh
- **Multiple environments**: Production, Staging, Test
- **Diverse technology stack**:
  - Application servers (NestJS, Node.js)
  - Database servers (MongoDB, PostgreSQL)
  - Web servers (Nginx)
  - Container orchestration (Docker)

**Th√°ch th·ª©c hi·ªán t·∫°i**:

```mermaid
graph TB
    subgraph "CURRENT STATE - Blind Spots"
        P1[‚ùå No Centralized Monitoring<br/>Logs scattered across 31 servers]
        P2[‚ùå Reactive Troubleshooting<br/>Users report issues first]
        P3[‚ùå Long MTTR<br/>Mean Time To Resolution: hours]
        P4[‚ùå No Observability<br/>Can't trace request flows]
        P5[‚ùå Manual Log Analysis<br/>SSH to each server]
        P6[‚ùå No Proactive Alerts<br/>Discover issues too late]
    end
    
    subgraph "IMPACT"
        I1[üí∞ Revenue Loss<br/>Downtime costs]
        I2[üòû Poor User Experience<br/>Slow responses, errors]
        I3[‚è∞ Team Burnout<br/>Firefighting mode]
        I4[üìâ SLA Violations<br/>99.9% uptime at risk]
    end
    
    P1 --> I1
    P2 --> I2
    P3 --> I3
    P4 --> I4
    P5 --> I3
    P6 --> I1
    
    style P1 fill:#ffcccc
    style P2 fill:#ffcccc
    style P3 fill:#ffcccc
    style P4 fill:#ffcccc
    style P5 fill:#ffcccc
    style P6 fill:#ffcccc
    style I1 fill:#ff9999
    style I2 fill:#ff9999
    style I3 fill:#ff9999
    style I4 fill:#ff9999
```

### 1.2. V·∫•n ƒë·ªÅ C·ª• th·ªÉ

#### V·∫•n ƒë·ªÅ 1: Kh√¥ng c√≥ T·∫ßm nh√¨n T·ªïng th·ªÉ (No Visibility)

**Tri·ªáu ch·ª©ng**:
- Kh√¥ng bi·∫øt server n√†o ƒëang c√≥ v·∫•n ƒë·ªÅ
- Kh√¥ng bi·∫øt CPU/RAM/Disk usage c·ªßa t·ª´ng server
- Kh√¥ng bi·∫øt application c√≥ l·ªói hay kh√¥ng

**V√≠ d·ª• th·ª±c t·∫ø**:
```
User: "API ch·∫≠m qu√°!"
DevOps: "Ch·∫≠m ·ªü ƒë√¢u? Server n√†o? Endpoint n√†o?"
‚Üí Ph·∫£i SSH v√†o 10 servers ƒë·ªÉ check logs
‚Üí M·∫•t 2 gi·ªù m·ªõi t√¨m ra: MongoDB connection pool ƒë·∫ßy
```

#### V·∫•n ƒë·ªÅ 2: Troubleshooting M·∫•t Th·ªùi gian

**Workflow hi·ªán t·∫°i**:
```mermaid
sequenceDiagram
    participant U as User
    participant D as DevOps
    participant S1 as Server 1
    participant S2 as Server 2
    participant S3 as Server 3
    
    U->>D: "API l·ªói 500!"
    D->>D: ƒêo√°n xem server n√†o c√≥ v·∫•n ƒë·ªÅ
    D->>S1: SSH + tail -f /var/log/app.log
    S1-->>D: Kh√¥ng th·∫•y l·ªói
    D->>S2: SSH + tail -f /var/log/app.log
    S2-->>D: Kh√¥ng th·∫•y l·ªói
    D->>S3: SSH + tail -f /var/log/app.log
    S3-->>D: T√¨m th·∫•y l·ªói!
    
    Note over D: M·∫•t 30 ph√∫t ch·ªâ ƒë·ªÉ t√¨m log
```

**Th·ªùi gian trung b√¨nh**:
- Ph√°t hi·ªán v·∫•n ƒë·ªÅ: 10-30 ph√∫t (user b√°o)
- T√¨m root cause: 1-3 gi·ªù
- Fix v√† deploy: 30 ph√∫t - 2 gi·ªù
- **MTTR (Mean Time To Resolution): 2-5 gi·ªù**

#### V·∫•n ƒë·ªÅ 3: Kh√¥ng Proactive

**Hi·ªán tr·∫°ng**:
- ‚ùå Kh√¥ng c√≥ alerts khi disk ƒë·∫ßy
- ‚ùå Kh√¥ng bi·∫øt khi n√†o CPU spike
- ‚ùå Kh√¥ng bi·∫øt database slow queries
- ‚ùå Ph√°t hi·ªán v·∫•n ƒë·ªÅ khi user complain

**H·∫≠u qu·∫£**:
```
Timeline c·ªßa m·ªôt incident:
10:00 - Disk usage 95% (kh√¥ng ai bi·∫øt)
10:30 - Disk full, app crash (kh√¥ng ai bi·∫øt)
11:00 - Users b√°o "website down"
11:05 - DevOps b·∫Øt ƒë·∫ßu investigate
12:00 - T√¨m ra nguy√™n nh√¢n: disk full
12:30 - Clean up disk, restart app

‚Üí Downtime: 2.5 gi·ªù
‚Üí C√≥ th·ªÉ tr√°nh ƒë∆∞·ª£c n·∫øu c√≥ alert khi disk 90%
```

---

## 2. Y√™u c·∫ßu H·ªá th·ªëng Monitoring

### 2.1. Functional Requirements

| ID | Requirement | Priority | Rationale |
|----|-------------|----------|-----------|
| **FR-1** | Thu th·∫≠p metrics t·ª´ 31+ servers | Critical | C·∫ßn visibility to√†n b·ªô h·∫° t·∫ßng |
| **FR-2** | T·ªïng h·ª£p logs t·ª´ t·∫•t c·∫£ sources | Critical | Centralized logging cho troubleshooting |
| **FR-3** | Distributed tracing cho APIs | High | Debug microservices performance |
| **FR-4** | Real-time alerting (Telegram) | Critical | Proactive incident detection |
| **FR-5** | Unified dashboard (single pane of glass) | High | Gi·∫£m context switching |
| **FR-6** | Correlation: metrics ‚Üî logs ‚Üî traces | High | Faster root cause analysis |
| **FR-7** | Historical data (15 days metrics, 7 days logs) | Medium | Trend analysis, capacity planning |
| **FR-8** | Self-hosted (on-premise) | Critical | Data sovereignty, cost control |

### 2.2. Non-Functional Requirements

| Category | Requirement | Target |
|----------|-------------|--------|
| **Performance** | Query response time | < 3 seconds |
| **Scalability** | Support growth to 100+ servers | Horizontal scaling |
| **Availability** | Monitoring system uptime | 99.9% |
| **Usability** | Learning curve for team | < 1 week |
| **Cost** | Total cost of ownership | < $500/month |
| **Maintainability** | DevOps effort | < 4 hours/week |

### 2.3. Key Metrics to Monitor

**Infrastructure (Golden Signals)**:
- **Latency**: Response time (p50, p95, p99)
- **Traffic**: Request rate (req/sec)
- **Errors**: Error rate (%)
- **Saturation**: CPU, Memory, Disk, Network usage

**Application**:
- HTTP request duration
- HTTP status codes (2xx, 4xx, 5xx)
- Database query duration
- External API call duration

**Database**:
- Connection pool usage
- Slow queries (> 1s)
- Replication lag
- Cache hit ratio

---

## 3. Ph√¢n lo·∫°i ƒê·ªëi t∆∞·ª£ng

### 3.1. ƒê·ªëi t∆∞·ª£ng V·∫≠t l√Ω (Physical Objects)

```mermaid
graph TB
    subgraph "PHYSICAL INFRASTRUCTURE"
        subgraph "Monitoring Server"
            MS[Physical/Virtual Machine<br/>IP: 10.170.100.X<br/>16GB RAM, 8 CPU, 500GB SSD<br/>OS: Ubuntu 22.04]
        end
        
        subgraph "Monitored Servers - Group 1"
            APP1[App Server 01<br/>IP: 10.170.100.210<br/>8GB RAM, 4 CPU]
            APP2[App Server 02<br/>IP: 10.170.100.181<br/>8GB RAM, 4 CPU]
        end
        
        subgraph "Monitored Servers - Group 2"
            DB1[MongoDB Server<br/>IP: 10.170.100.88<br/>16GB RAM, 8 CPU]
            DB2[PostgreSQL Server<br/>IP: 10.170.100.24<br/>16GB RAM, 8 CPU]
        end
        
        subgraph "Monitored Servers - Group 3"
            WEB1[Web Server 01<br/>IP: 10.170.100.180<br/>4GB RAM, 2 CPU]
            WEB2[Web Server 02<br/>IP: 10.170.100.182<br/>4GB RAM, 2 CPU]
        end
        
        subgraph "Network"
            NET[Corporate Network<br/>10.170.100.0/24<br/>1 Gbps bandwidth]
        end
    end
    
    MS -.-> NET
    APP1 -.-> NET
    APP2 -.-> NET
    DB1 -.-> NET
    DB2 -.-> NET
    WEB1 -.-> NET
    WEB2 -.-> NET
    
    style MS fill:#99ff99
    style APP1 fill:#e1f5ff
    style APP2 fill:#e1f5ff
    style DB1 fill:#fff9c4
    style DB2 fill:#fff9c4
    style WEB1 fill:#ffe0b2
    style WEB2 fill:#ffe0b2
```

**T·ªïng quan**:
- **1 Monitoring Server**: Ch·ª©a to√†n b·ªô LGTM Stack
- **31+ Monitored Servers**: C√°c m√°y ch·ªß ƒë∆∞·ª£c gi√°m s√°t
- **1 Corporate Network**: K·∫øt n·ªëi t·∫•t c·∫£ servers

### 3.2. ƒê·ªëi t∆∞·ª£ng Tri·ªÉn khai (Deployment Objects)

```mermaid
graph TB
    subgraph "DEPLOYMENT OBJECTS"
        subgraph "Monitoring Server Components"
            C1[Docker Container: Prometheus]
            C2[Docker Container: Grafana]
            C3[Docker Container: Loki]
            C4[Docker Container: Tempo]
            C5[Docker Container: OTEL Collector]
            C6[Docker Container: Alertmanager]
            C7[Docker Container: Promtail]
        end
        
        subgraph "Monitored Server Components"
            E1[Binary/Docker: Node Exporter]
            E2[Docker: cAdvisor]
            E3[Docker: MongoDB Exporter]
            E4[Docker: PostgreSQL Exporter]
            E5[Docker: Nginx Exporter]
        end
        
        subgraph "Application Components"
            A1[NestJS App with Pino Logger]
            A2[OpenTelemetry SDK]
        end
    end
    
    style C1 fill:#ff9999
    style C2 fill:#99ff99
    style C3 fill:#ffcc99
    style C4 fill:#cc99ff
    style E1 fill:#e1f5ff
    style E2 fill:#e1f5ff
    style A1 fill:#fff9c4
```

**Ph√¢n lo·∫°i theo vai tr√≤**:

| Object Type | Deployment Location | Count | Purpose |
|-------------|---------------------|-------|---------|
| **LGTM Stack** | Monitoring Server | 7 containers | Core monitoring platform |
| **System Exporters** | All Monitored Servers | 31+ instances | Collect system metrics |
| **Service Exporters** | Specific Servers | Variable | Collect service-specific metrics |
| **Application Instrumentation** | Application Servers | Per app | Generate traces & logs |

### 3.3. ƒê·ªëi t∆∞·ª£ng ƒê∆∞·ª£c Monitor (Monitored Objects)

```mermaid
mindmap
  root((Monitored<br/>Objects))
    Infrastructure
      Servers
        CPU Usage
        Memory Usage
        Disk I/O
        Network Traffic
      Network
        Bandwidth
        Latency
        Packet Loss
    Platform
      Containers
        Container CPU
        Container Memory
        Container Restarts
      Databases
        MongoDB
          Connections
          Operations/sec
          Replication Lag
        PostgreSQL
          Connections
          Slow Queries
          Table Stats
    Application
      HTTP APIs
        Request Rate
        Response Time
        Error Rate
      Business Logic
        User Registrations
        Transactions
        Payment Success Rate
      Dependencies
        External API Calls
        Cache Hit Ratio
    User Experience
      Frontend
        Page Load Time
        JavaScript Errors
      End-to-End
        Transaction Success
        User Journey
```

**Chi ti·∫øt t·ª´ng layer**:

#### Layer 1: Infrastructure Monitoring

**ƒê·ªëi t∆∞·ª£ng**: Physical/Virtual machines

**Metrics thu th·∫≠p**:
- CPU: usage, load average, context switches
- Memory: used, available, swap
- Disk: usage, I/O operations, latency
- Network: bytes in/out, errors, dropped packets

**Tool**: Node Exporter

#### Layer 2: Platform Monitoring

**ƒê·ªëi t∆∞·ª£ng**: Containers, Databases, Message Queues

**Metrics thu th·∫≠p**:
- **Containers** (cAdvisor):
  - Per-container CPU/Memory
  - Container restarts
  - Image versions
  
- **MongoDB** (MongoDB Exporter):
  - Connection pool usage
  - Operations per second (insert, update, delete)
  - Replication lag
  - Slow queries
  
- **PostgreSQL** (PostgreSQL Exporter):
  - Active connections
  - Transaction rate
  - Table/Index sizes
  - Slow queries (> 1s)

#### Layer 3: Application Monitoring

**ƒê·ªëi t∆∞·ª£ng**: APIs, Services, Business Logic

**Metrics thu th·∫≠p**:
- **HTTP Requests**:
  - Rate (requests/sec)
  - Duration (p50, p95, p99)
  - Status codes (2xx, 4xx, 5xx)
  
- **Traces**:
  - Request flow across services
  - Span duration per operation
  - Error traces
  
- **Logs**:
  - Application errors
  - Business events
  - Audit logs

**Tool**: OpenTelemetry SDK, Pino Logger

#### Layer 4: User Experience Monitoring

**ƒê·ªëi t∆∞·ª£ng**: End-user interactions

**Metrics thu th·∫≠p** (future scope):
- Frontend performance (page load time)
- Real User Monitoring (RUM)
- Synthetic monitoring (uptime checks)

---

## 4. So s√°nh Gi·∫£i ph√°p

### 4.1. C√°c Gi·∫£i ph√°p ƒê∆∞·ª£c Xem x√©t

```mermaid
graph LR
    subgraph "EVALUATION"
        OPT1[Option 1:<br/>LGTM Stack<br/>Open-Source]
        OPT2[Option 2:<br/>ELK Stack<br/>Open-Source]
        OPT3[Option 3:<br/>Datadog<br/>Commercial SaaS]
        OPT4[Option 4:<br/>SigNoz<br/>Open-Source]
    end
    
    EVAL{Evaluation<br/>Criteria}
    
    EVAL --> OPT1
    EVAL --> OPT2
    EVAL --> OPT3
    EVAL --> OPT4
    
    OPT1 --> DECISION[‚úÖ SELECTED:<br/>LGTM Stack]
    
    style OPT1 fill:#99ff99
    style DECISION fill:#66ff66
```

### 4.2. Ma tr·∫≠n So s√°nh Chi ti·∫øt

| Criteria | LGTM Stack | ELK Stack | Datadog | SigNoz | Weight |
|----------|------------|-----------|---------|---------|--------|
| **Cost** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Free | ‚≠ê‚≠ê‚≠ê‚≠ê Free | ‚≠ê‚≠ê $$$$ | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Free | 25% |
| **Unified Observability** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê M+L+T | ‚≠ê‚≠ê‚≠ê Logs only | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê M+L+T | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê M+L+T | 20% |
| **Ease of Setup** | ‚≠ê‚≠ê‚≠ê‚≠ê Moderate | ‚≠ê‚≠ê‚≠ê Complex | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Easy | ‚≠ê‚≠ê‚≠ê‚≠ê Moderate | 15% |
| **Resource Usage** | ‚≠ê‚≠ê‚≠ê‚≠ê Low | ‚≠ê‚≠ê High | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê N/A | ‚≠ê‚≠ê‚≠ê‚≠ê Low | 15% |
| **Scalability** | ‚≠ê‚≠ê‚≠ê‚≠ê Good | ‚≠ê‚≠ê‚≠ê‚≠ê Good | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Excellent | ‚≠ê‚≠ê‚≠ê Moderate | 10% |
| **Community Support** | ‚≠ê‚≠ê‚≠ê‚≠ê Strong | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Huge | ‚≠ê‚≠ê‚≠ê‚≠ê Good | ‚≠ê‚≠ê‚≠ê Growing | 5% |
| **Data Sovereignty** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Self-hosted | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Self-hosted | ‚≠ê SaaS only | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Self-hosted | 10% |
| **Total Score** | **4.45** | **3.55** | **3.85** | **4.15** | **100%** |

### 4.3. Ph√¢n t√≠ch Chi ti·∫øt T·ª´ng Gi·∫£i ph√°p

#### Option 1: LGTM Stack (Loki, Grafana, Tempo, Prometheus)

**∆Øu ƒëi·ªÉm** ‚úÖ:
- ‚úÖ **Cost-effective**: Ho√†n to√†n mi·ªÖn ph√≠, kh√¥ng c√≥ licensing fees
- ‚úÖ **Unified Observability**: Metrics + Logs + Traces trong m·ªôt platform
- ‚úÖ **Resource Efficient**: Loki ch·ªâ index labels, kh√¥ng index log content ‚Üí ti·∫øt ki·ªám storage
- ‚úÖ **Grafana Integration**: Single pane of glass, correlation gi·ªØa M-L-T
- ‚úÖ **Cloud-Native**: Thi·∫øt k·∫ø cho Kubernetes, containers
- ‚úÖ **OpenTelemetry Support**: Standard instrumentation
- ‚úÖ **Self-hosted**: Full control, data sovereignty

**Nh∆∞·ª£c ƒëi·ªÉm** ‚ùå:
- ‚ùå **Complexity**: C·∫ßn qu·∫£n l√Ω 7 components
- ‚ùå **Learning Curve**: Team c·∫ßn h·ªçc LogQL, PromQL, TraceQL
- ‚ùå **No Official Support**: Community support only
- ‚ùå **Manual HA Setup**: C·∫ßn t·ª± config high availability

**Total Cost of Ownership (3 nƒÉm)**:
```
Infrastructure: $0 (s·ª≠ d·ª•ng existing servers)
Licensing: $0
DevOps Time: 4 hours/week √ó 52 weeks √ó 3 years √ó $50/hour = $31,200
Training: 1 week √ó 3 people √ó $400/day √ó 5 days = $6,000
Total: $37,200
```

#### Option 2: ELK Stack (Elasticsearch, Logstash, Kibana)

**∆Øu ƒëi·ªÉm** ‚úÖ:
- ‚úÖ **Powerful Search**: Full-text search tr√™n logs
- ‚úÖ **Mature**: ƒê√£ t·ªìn t·∫°i > 10 nƒÉm, battle-tested
- ‚úÖ **Rich Visualizations**: Kibana dashboards r·∫•t m·∫°nh
- ‚úÖ **Large Community**: Nhi·ªÅu tutorials, plugins

**Nh∆∞·ª£c ƒëi·ªÉm** ‚ùå:
- ‚ùå **Logs Only**: Kh√¥ng c√≥ native metrics/traces (c·∫ßn th√™m Metricbeat, APM)
- ‚ùå **Resource Hungry**: Elasticsearch c·∫ßn nhi·ªÅu RAM (8-16GB minimum)
- ‚ùå **Complex Setup**: Logstash pipelines ph·ª©c t·∫°p
- ‚ùå **Licensing Changes**: Elastic License (kh√¥ng ph·∫£i pure open-source)
- ‚ùå **High Storage Cost**: Index to√†n b·ªô log content

**Total Cost of Ownership (3 nƒÉm)**:
```
Infrastructure: $200/month √ó 36 months = $7,200 (larger servers)
Licensing: $0 (basic tier)
DevOps Time: 6 hours/week √ó 52 weeks √ó 3 years √ó $50/hour = $46,800
Training: 2 weeks √ó 3 people √ó $400/day √ó 5 days = $12,000
Total: $66,000
```

#### Option 3: Datadog (Commercial SaaS)

**∆Øu ƒëi·ªÉm** ‚úÖ:
- ‚úÖ **All-in-One**: Metrics, Logs, Traces, RUM, Synthetics
- ‚úÖ **Easy Setup**: Agent install, done
- ‚úÖ **600+ Integrations**: Out-of-the-box dashboards
- ‚úÖ **24/7 Support**: Enterprise support
- ‚úÖ **No Maintenance**: Fully managed

**Nh∆∞·ª£c ƒëi·ªÉm** ‚ùå:
- ‚ùå **Expensive**: $15-31/host/month + $0.10/GB logs
- ‚ùå **Vendor Lock-in**: Proprietary platform
- ‚ùå **Data Sovereignty**: Data stored in Datadog cloud
- ‚ùå **Unpredictable Costs**: Costs scale with usage
- ‚ùå **Limited Customization**: Can't modify platform

**Total Cost of Ownership (3 nƒÉm)**:
```
Datadog Pricing (31 hosts):
- Infrastructure Monitoring: $15/host/month √ó 31 √ó 36 = $16,740
- Log Management: $0.10/GB √ó 100GB/day √ó 30 days √ó 36 = $10,800
- APM: $31/host/month √ó 10 √ó 36 = $11,160
Total: $38,700

DevOps Time: 1 hour/week √ó 52 weeks √ó 3 years √ó $50/hour = $7,800
Training: 3 days √ó 3 people √ó $400/day = $3,600
Total: $50,100
```

#### Option 4: SigNoz (Open-Source APM)

**∆Øu ƒëi·ªÉm** ‚úÖ:
- ‚úÖ **Unified Platform**: Metrics, Logs, Traces
- ‚úÖ **Open-Source**: Free, self-hosted
- ‚úÖ **Modern UI**: Better UX than Grafana
- ‚úÖ **ClickHouse Backend**: Fast queries

**Nh∆∞·ª£c ƒëi·ªÉm** ‚ùå:
- ‚ùå **Young Project**: Less mature (started 2021)
- ‚ùå **Smaller Community**: Fewer resources
- ‚ùå **Limited Integrations**: Fewer exporters than Prometheus
- ‚ùå **ClickHouse Complexity**: New database to manage

### 4.4. Decision Matrix

```mermaid
graph TB
    START[Start Evaluation]
    
    Q1{Budget < $10k/year?}
    Q2{Need Metrics + Logs + Traces?}
    Q3{Self-hosted required?}
    Q4{Team has DevOps skills?}
    Q5{Resource constrained?}
    
    START --> Q1
    Q1 -->|No| DATADOG[Datadog]
    Q1 -->|Yes| Q2
    Q2 -->|No, Logs only| ELK[ELK Stack]
    Q2 -->|Yes| Q3
    Q3 -->|No| DATADOG
    Q3 -->|Yes| Q4
    Q4 -->|No| SIGNOZ[SigNoz<br/>Easier setup]
    Q4 -->|Yes| Q5
    Q5 -->|Yes| LGTM[‚úÖ LGTM Stack<br/>Resource efficient]
    Q5 -->|No| SIGNOZ
    
    style LGTM fill:#66ff66
    style DATADOG fill:#ffcccc
    style ELK fill:#fff9c4
    style SIGNOZ fill:#e1f5ff
```

---

## 5. Quy·∫øt ƒë·ªãnh L·ª±a ch·ªçn

### 5.1. L√Ω do Ch·ªçn LGTM Stack

**Quy·∫øt ƒë·ªãnh**: ‚úÖ **LGTM Stack (Loki + Grafana + Tempo + Prometheus)**

**Rationale**:

1. **Cost-Effective** (Weight: 25%)
   - Zero licensing cost
   - TCO th·∫•p nh·∫•t trong 3 nƒÉm: $37,200 vs $66,000 (ELK) vs $50,100 (Datadog)
   - Ph√π h·ª£p v·ªõi budget constraint

2. **Unified Observability** (Weight: 20%)
   - ƒê√°p ·ª©ng ƒë·∫ßy ƒë·ªß 3 pillars: Metrics + Logs + Traces
   - Correlation gi·ªØa M-L-T trong Grafana
   - ELK ch·ªâ m·∫°nh v·ªÅ Logs

3. **Resource Efficiency** (Weight: 15%)
   - Loki: Label-based indexing ‚Üí storage nh·ªè h∆°n ELK 10-100x
   - Prometheus: Efficient TSDB
   - C√≥ th·ªÉ ch·∫°y tr√™n existing infrastructure

4. **Data Sovereignty** (Weight: 10%)
   - Self-hosted, full control
   - Compliance v·ªõi data regulations
   - Datadog kh√¥ng ƒë√°p ·ª©ng requirement n√†y

5. **Team Skills** (Weight: 15%)
   - Team ƒë√£ c√≥ experience v·ªõi Prometheus
   - Grafana familiar v·ªõi team
   - Learning curve ch·∫•p nh·∫≠n ƒë∆∞·ª£c (< 1 week)

6. **Community & Ecosystem** (Weight: 5%)
   - CNCF projects (Prometheus, Grafana)
   - Large community
   - Many integrations

### 5.2. Trade-offs Ch·∫•p nh·∫≠n

**Trade-off 1: Complexity vs Cost**
- ‚úÖ Ch·∫•p nh·∫≠n: Qu·∫£n l√Ω 7 components
- üí∞ L·ª£i √≠ch: Ti·∫øt ki·ªám $13k-29k/3 nƒÉm so v·ªõi alternatives

**Trade-off 2: Self-managed vs Managed**
- ‚úÖ Ch·∫•p nh·∫≠n: T·ª± maintain (4 hours/week)
- üîí L·ª£i √≠ch: Data sovereignty, full control

**Trade-off 3: Community Support vs Enterprise Support**
- ‚úÖ Ch·∫•p nh·∫≠n: Kh√¥ng c√≥ 24/7 support
- üìö L·ª£i √≠ch: Large community, extensive documentation

### 5.3. Risk Mitigation

| Risk | Mitigation Strategy |
|------|---------------------|
| **Complexity** | - Comprehensive documentation<br/>- Team training (1 week)<br/>- Start with core features, expand gradually |
| **No Official Support** | - Active community forums<br/>- Grafana Labs consulting (if needed)<br/>- Internal knowledge base |
| **Single Point of Failure** | - Phase 2: Implement HA architecture<br/>- Regular backups<br/>- Disaster recovery plan |
| **Scalability Limits** | - Monitor resource usage<br/>- Plan for horizontal scaling (Thanos)<br/>- Capacity planning quarterly |

---

## 6. Roadmap Tri·ªÉn khai

### 6.1. Timeline T·ªïng quan

```mermaid
gantt
    title LGTM Stack Deployment Roadmap
    dateFormat YYYY-MM-DD
    section Phase 1: Foundation
    Setup Monitoring Server           :p1-1, 2026-01-10, 3d
    Deploy LGTM Stack                  :p1-2, after p1-1, 2d
    Configure Datasources              :p1-3, after p1-2, 1d
    
    section Phase 2: Metrics
    Deploy Node Exporters (31 servers) :p2-1, after p1-3, 5d
    Deploy cAdvisor                    :p2-2, after p2-1, 2d
    Deploy DB Exporters                :p2-3, after p2-2, 2d
    Configure Alert Rules              :p2-4, after p2-3, 2d
    
    section Phase 3: Logs
    Configure Promtail                 :p3-1, after p2-4, 2d
    Integrate App Logging (Pino-Loki)  :p3-2, after p3-1, 3d
    Setup Log Parsing Pipelines        :p3-3, after p3-2, 2d
    
    section Phase 4: Traces
    Instrument Applications (OTEL)     :p4-1, after p3-3, 5d
    Configure Tempo                    :p4-2, after p4-1, 1d
    Setup Service Graphs               :p4-3, after p4-2, 1d
    
    section Phase 5: Dashboards
    Import Pre-built Dashboards        :p5-1, after p4-3, 2d
    Create Custom Dashboards           :p5-2, after p5-1, 3d
    Setup Alertmanager (Telegram)      :p5-3, after p5-2, 1d
    
    section Phase 6: Training & Docs
    Team Training                      :p6-1, after p5-3, 5d
    Write Runbooks                     :p6-2, after p6-1, 3d
    Go Live                            :milestone, after p6-2, 0d
```

**Total Duration**: ~6 weeks (30 working days)

### 6.2. Phase 1: Foundation (Week 1)

**Objective**: Setup monitoring server v√† LGTM Stack core

**Tasks**:

| Day | Task | Owner | Deliverable |
|-----|------|-------|-------------|
| 1 | Provision monitoring server | DevOps | Server ready (16GB RAM, 8 CPU) |
| 1-2 | Install Docker & Docker Compose | DevOps | Docker running |
| 2-3 | Clone repo, configure .env | DevOps | Config files ready |
| 3 | Deploy LGTM Stack (docker-compose up) | DevOps | 7 containers running |
| 4 | Configure Grafana datasources | DevOps | Prometheus, Loki, Tempo connected |
| 5 | Verify stack health | DevOps | All services UP |

**Success Criteria**:
- ‚úÖ Grafana accessible at http://monitoring-server:3000
- ‚úÖ Prometheus scraping itself
- ‚úÖ Loki receiving test logs
- ‚úÖ Tempo ready for traces

### 6.3. Phase 2: Metrics Collection (Week 2)

**Objective**: Deploy exporters v√† thu th·∫≠p metrics

**Tasks**:

| Day | Task | Servers | Deliverable |
|-----|------|---------|-------------|
| 1-3 | Deploy Node Exporter | All 31 servers | System metrics flowing |
| 3-4 | Deploy cAdvisor | 10 container hosts | Container metrics flowing |
| 4-5 | Deploy MongoDB Exporter | 3 MongoDB servers | DB metrics flowing |
| 5 | Deploy PostgreSQL Exporter | 2 PostgreSQL servers | DB metrics flowing |
| 5 | Configure Prometheus targets (JSON files) | Monitoring server | All targets scraped |

**Success Criteria**:
- ‚úÖ All 31 Node Exporters UP in Prometheus
- ‚úÖ cAdvisor metrics visible
- ‚úÖ Database metrics visible
- ‚úÖ No scrape errors

### 6.4. Phase 3: Logs Aggregation (Week 3)

**Objective**: Centralize logs t·ª´ t·∫•t c·∫£ sources

**Tasks**:

| Day | Task | Deliverable |
|-----|------|-------------|
| 1 | Configure Promtail for Docker logs | Container logs in Loki |
| 2 | Configure Promtail for system logs | System logs in Loki |
| 3-4 | Integrate Pino-Loki in NestJS apps | App logs in Loki |
| 5 | Setup log parsing pipelines | Structured logs |

**Success Criteria**:
- ‚úÖ Docker container logs visible in Grafana
- ‚úÖ System logs (syslog, auth.log) visible
- ‚úÖ Application logs with trace_id correlation

### 6.5. Phase 4: Distributed Tracing (Week 4)

**Objective**: Instrument applications v·ªõi OpenTelemetry

**Tasks**:

| Day | Task | Deliverable |
|-----|------|-------------|
| 1-3 | Add OTEL SDK to NestJS apps | Traces generated |
| 3-4 | Configure OTEL Collector | Traces flowing to Tempo |
| 4 | Setup span metrics generation | Span metrics in Prometheus |
| 5 | Test trace correlation with logs | Trace ID in logs |

**Success Criteria**:
- ‚úÖ Traces visible in Grafana Tempo
- ‚úÖ Span metrics available
- ‚úÖ Click trace ‚Üí jump to logs works

### 6.6. Phase 5: Dashboards & Alerting (Week 5)

**Objective**: Setup dashboards v√† alerts

**Tasks**:

| Day | Task | Deliverable |
|-----|------|-------------|
| 1 | Import Node Exporter Full dashboard | System metrics dashboard |
| 1 | Import MongoDB dashboard | MongoDB dashboard |
| 2 | Create custom application dashboard | App metrics dashboard |
| 3 | Configure alert rules (CPU, Disk, Memory) | Alert rules active |
| 4 | Setup Alertmanager with Telegram | Alerts sent to Telegram |
| 5 | Test alerting (trigger test alerts) | Alerts working |

**Success Criteria**:
- ‚úÖ 5+ dashboards created
- ‚úÖ 10+ alert rules configured
- ‚úÖ Telegram notifications working

### 6.7. Phase 6: Training & Documentation (Week 6)

**Objective**: Train team v√† document processes

**Tasks**:

| Day | Task | Deliverable |
|-----|------|-------------|
| 1-2 | Team training: Grafana basics | Team can navigate Grafana |
| 2-3 | Team training: PromQL, LogQL | Team can write queries |
| 3-4 | Write runbooks for common issues | 10+ runbooks |
| 4-5 | Write operational procedures | Ops docs complete |
| 5 | Go-live review & sign-off | Production ready |

**Success Criteria**:
- ‚úÖ All team members trained
- ‚úÖ Runbooks documented
- ‚úÖ On-call rotation defined
- ‚úÖ Escalation procedures clear

### 6.8. Success Metrics (Post Go-Live)

**Track these KPIs**:

| Metric | Baseline (Before) | Target (3 months) |
|--------|-------------------|-------------------|
| **MTTD** (Mean Time To Detect) | 10-30 min | < 5 min |
| **MTTR** (Mean Time To Resolve) | 2-5 hours | < 1 hour |
| **Alert Accuracy** | N/A | > 90% |
| **Dashboard Usage** | 0 views/day | > 50 views/day |
| **Incident Postmortems** | 0% | 100% |
| **Proactive Incidents** | 0% | > 50% |

---

## üìö T√†i li·ªáu Li√™n quan

- **[DEPLOYMENT.md](./DEPLOYMENT.md)** - H∆∞·ªõng d·∫´n tri·ªÉn khai chi ti·∫øt
- **[NETWORK_ARCHITECTURE.md](./NETWORK_ARCHITECTURE.md)** - Ki·∫øn tr√∫c m·∫°ng v√† data flows
- **[INCIDENT_RESPONSE.md](./INCIDENT_RESPONSE.md)** - Quy tr√¨nh x·ª≠ l√Ω s·ª± c·ªë (s·∫Ω t·∫°o ti·∫øp)

---

**Version**: 1.0.0  
**Last Updated**: 2026-01-09  
**Maintainer**: DevOps Team
